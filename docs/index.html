<!doctype html>
<html lang="en">
  <head>
	<meta name="generator" content="Hugo 0.89.4" />
    <meta charset="utf-8">
    <meta content="IE=edge,chrome=1" http-equiv="X-UA-Compatible">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <title>
      Handover Document
    </title>
    








<link href='/DocRankingWebsite/styles/screen.min.dbadab296f1baed94fc5109ecef71e620384615bbb9c2c5fb0f0c9f10f900ab9.css' rel="stylesheet" media="screen" integrity="sha256-262rKW8brtlPxRCezvceYgOEYVu7nCxfsPDJ8Q&#43;QCrk=" />
<link href='/DocRankingWebsite/styles/print.min.e0db9e70d333a92515c28264c527870199194686a03c044b826e9827ec960b9d.css' rel="stylesheet" media="print" integrity="sha256-4NuecNMzqSUVwoJkxSeHAZkZRoagPARLgm6YJ&#43;yWC50=" />


    




  

<script src="/DocRankingWebsite/js/index.e4619e460f1bf032b63c9e8bf4c022ec118c03648d58c61e6a8402ae9e194017.js" integrity="sha256-5GGeRg8b8DK2PJ6L9MAi7BGMA2SNWMYeaoQCrp4ZQBc=" defer></script>

    


    <style>
      [x-cloak] {
      display: none !important;
      }
    </style>
  </head>
  <body class="index" x-data="{ navOpen: false }" x-cloak>
    <a href="#" id="nav-button" @click="navOpen = !navOpen" :class="{'open': navOpen }">
      <span>
        NAV
        <img src='/DocRankingWebsite/images/navbar.png'/>
      </span>
    </a>
    <div class="toc-wrapper" :class="{'open': navOpen }">
      
       <img src='/DocRankingWebsite/images/logo.png' class="logo" />
      
        <div class="search" x-data="searchController">
          <input x-model.debounce.100ms="query" type="search" class="search" id="input-search" placeholder='Search'>
          <ul class="search-results visible" x-show="results.length > 0" x-transition.duration.700ms >
            <template x-for="item in results">
              <li>
                <a x-text="item.title"></a>
              </li>
            </template>
          </ul>
        </div>
      
      
  
  
    
  
  <ul id="toc" class="toc-list-h1" x-data="tocController" x-init="load([{&#34;id&#34;:&#34;intro&#34;,&#34;level&#34;:1,&#34;sub&#34;:[{&#34;id&#34;:&#34;about&#34;,&#34;level&#34;:2,&#34;title&#34;:&#34;About&#34;},{&#34;id&#34;:&#34;motivation&#34;,&#34;level&#34;:2,&#34;sub&#34;:[],&#34;title&#34;:&#34;Motivation&#34;}],&#34;title&#34;:&#34;Intro&#34;},{&#34;id&#34;:&#34;metrics&#34;,&#34;level&#34;:1,&#34;sub&#34;:[],&#34;title&#34;:&#34;Metrics&#34;},{&#34;id&#34;:&#34;timeline&#34;,&#34;level&#34;:1,&#34;sub&#34;:[{&#34;id&#34;:&#34;exploratory-data-analysis&#34;,&#34;level&#34;:2,&#34;sub&#34;:[{&#34;id&#34;:&#34;answer-ids&#34;,&#34;level&#34;:3,&#34;title&#34;:&#34;Answer ids&#34;},{&#34;id&#34;:&#34;dataset-imbalance&#34;,&#34;level&#34;:3,&#34;title&#34;:&#34;Dataset imbalance&#34;},{&#34;id&#34;:&#34;pca&#34;,&#34;level&#34;:3,&#34;title&#34;:&#34;PCA&#34;}],&#34;title&#34;:&#34;Exploratory Data Analysis&#34;},{&#34;id&#34;:&#34;document-ranking-model&#34;,&#34;level&#34;:2,&#34;sub&#34;:[{&#34;id&#34;:&#34;preprocessing-for-bert&#34;,&#34;level&#34;:3,&#34;title&#34;:&#34;Preprocessing for BERT&#34;},{&#34;id&#34;:&#34;bert-with-one-head&#34;,&#34;level&#34;:3,&#34;title&#34;:&#34;BERT with one head&#34;},{&#34;id&#34;:&#34;bert-with-dual-encoder&#34;,&#34;level&#34;:3,&#34;title&#34;:&#34;BERT with Dual Encoder&#34;}],&#34;title&#34;:&#34;Document Ranking Model&#34;},{&#34;id&#34;:&#34;pretraining-with-barlow-twins&#34;,&#34;level&#34;:2,&#34;sub&#34;:[{&#34;id&#34;:&#34;theory&#34;,&#34;level&#34;:3,&#34;title&#34;:&#34;Theory&#34;},{&#34;id&#34;:&#34;code&#34;,&#34;level&#34;:3,&#34;title&#34;:&#34;Code&#34;},{&#34;id&#34;:&#34;implementation&#34;,&#34;level&#34;:3,&#34;title&#34;:&#34;Implementation&#34;}],&#34;title&#34;:&#34;Pretraining with Barlow Twins&#34;}],&#34;title&#34;:&#34;Timeline&#34;},{&#34;id&#34;:&#34;model&#34;,&#34;level&#34;:1,&#34;sub&#34;:[{&#34;id&#34;:&#34;bert-base&#34;,&#34;level&#34;:2,&#34;title&#34;:&#34;BERT Base&#34;},{&#34;id&#34;:&#34;barlow-head&#34;,&#34;level&#34;:2,&#34;title&#34;:&#34;Barlow Head&#34;},{&#34;id&#34;:&#34;ranking-head&#34;,&#34;level&#34;:2,&#34;title&#34;:&#34;Ranking Head&#34;},{&#34;id&#34;:&#34;pre-training-model&#34;,&#34;level&#34;:2,&#34;title&#34;:&#34;Pre-training Model&#34;},{&#34;id&#34;:&#34;training-model&#34;,&#34;level&#34;:2,&#34;sub&#34;:[],&#34;title&#34;:&#34;Training Model&#34;}],&#34;title&#34;:&#34;Model&#34;},{&#34;id&#34;:&#34;results&#34;,&#34;level&#34;:1,&#34;sub&#34;:[{&#34;sub&#34;:[{&#34;id&#34;:&#34;document-ranking-results&#34;,&#34;level&#34;:3,&#34;title&#34;:&#34;Document Ranking Results&#34;},{&#34;id&#34;:&#34;barlow-results&#34;,&#34;level&#34;:3,&#34;title&#34;:&#34;Barlow Results&#34;},{&#34;id&#34;:&#34;barlow-and-document-ranking&#34;,&#34;level&#34;:3,&#34;title&#34;:&#34;Barlow and Document Ranking&#34;}]}],&#34;title&#34;:&#34;Results&#34;},{&#34;id&#34;:&#34;discussion&#34;,&#34;level&#34;:1,&#34;sub&#34;:[{&#34;id&#34;:&#34;barlow&#34;,&#34;level&#34;:2,&#34;title&#34;:&#34;Barlow&#34;},{&#34;id&#34;:&#34;barlow-with-document-ranking-bert&#34;,&#34;level&#34;:2,&#34;sub&#34;:[],&#34;title&#34;:&#34;Barlow with Document Ranking BERT&#34;}],&#34;title&#34;:&#34;Discussion&#34;}])" @scroll.window="onScroll()">
    
    <template x-for="row in rows">
      <li>
        <a x-text="row.title" @click="click(row)" :href="`#${row.id}`" class="toc-link" x-bind="rowClass(row)"></a>
        
          <ul x-show="row.open" x-bind="transitions()" class="da-toc-list-h2">
            <template x-for="row in row.sub">
              <li>
                <a x-text="row.title" @click="click(row)" :href="`#${row.id}`" class="toc-link" x-bind="rowClass(row)"></a>
                
                  <ul x-show="row.open" x-bind="transitions()" class="toc-list-h3">
                    <template x-for="row in row.sub">
                      <li>
                        <a  x-text="row.title" @click="click(row)" :href="`#${row.id}`"  class="toc-link" x-bind="rowClass(row)"></a>
                      </li>
                    </template>
                  </ul>
                
              </li>
            </template>
          </ul>
        
      </li>
    </template>
  </ul>

      
      
      

    </div>
    <div class="page-wrapper">
      <div class="dark-box"></div>
      <div class="content">
        
  
    
  
    
  
    
  
    
  
    
  
    
  
  
    





<h1 id="intro">Intro</h1>



  



<h2 id="about">About</h2><p>This website is a Handover Document by <a href="https://github.com/mikkelfo">Mikkel Odgaard</a> and <a href="https://github.com/nklingen">Natasha Klingenbrunn</a> for a collaboration project between Denmark&rsquo;s Technical University and <a href="https://www.raffle.ai">Raffle.ai</a>. The <a href="https://github.com/nklingen/BERT_Question_Answering">final code</a> can be found on Github.</p>
<p>The website is broken down as follows:</p>
<ol>
<li>The metrics for this project are outlined.</li>
<li>Next, the Timeline section will discuss the evolution of our project and discuss the rationale behind given design choices. Barlow Twins will be explained both conceptually and implementation-wise.</li>
<li>The entire Model.py class will be broken down and explained in detail.</li>
<li>Finally, the discussion section will outline why believe our model was not successful, and touch upon our final thoughts.</li>
</ol>




  



<h2 id="motivation">Motivation</h2><p>Raffle.ai&rsquo;s BERT Document Ranking Model sees better performance on questions than on conversations. One hypothesis prior to beginning this project was that the BERT Ranking Model might project question and conversations into two seperate planes. Thus, while it may be very successful in one input-type, this does not translate to high performance for the other. The motivation behind this project is to introduce a pre-training step implementing Barlow Twins to push encodings together for similar questions and conversation.</p>
<p>Conceptually, Barlow twins aims to &ldquo;collect&rdquo; queries from the same answer id [think: high precision, but not necessarily accurate], while Document Ranking will put individual queries closer to the answer [think: high accuracy, but not necessarily precise]. Training the two loss functions one after the other will ideally encourage both accurate and precise results wherein one query type does not outperform the other.</p>
<p><img src="https://nklingen.github.io/DocRankingWebsite//images/nat.png" alt="nat"></p>
<p>Secondly, we would like to interpret the results on performance. Having a successful implementation of the pre-training model will allow us to analyse the downstream effects in the BERT Ranking Model. We hope to learn whether having more similar embeddings for questions and conversations has a positive effect on the model, or whether the model&rsquo;s own learned representations are better. While we did not find it likely to have a better initial performance (if projecting all inputs from the same answer_id together had a lower loss in the downstream task, the original model would have done it anyways), but hoped that we could see other indicators of pre-training benefitting the model, such as a quicker rate of converging, for example.</p>

  
    



  



<h1 id="metrics">Metrics</h1><blockquote>
<p>To compute top k accuracy</p>
</blockquote>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">k_accuracy</span>(scores, answer_ids, target_answer_ids, k):
    <span style="color:#66d9ef">assert</span>  k <span style="color:#f92672">&gt;=</span> <span style="color:#ae81ff">1</span>
    <span style="color:#66d9ef">if</span>  k <span style="color:#f92672">==</span> <span style="color:#ae81ff">1</span>:
        prediction_indices <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>argmax(scores, dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
        prediction_answer_ids <span style="color:#f92672">=</span> answer_ids[prediction_indices]
        <span style="color:#66d9ef">return</span> (prediction_answer_ids <span style="color:#f92672">==</span> target_answer_ids)
    <span style="color:#66d9ef">elif</span>  k <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">1</span>:
        prediction_indices <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>topk(scores, k)<span style="color:#f92672">.</span>indices
        prediction_answer_ids <span style="color:#f92672">=</span> answer_ids[prediction_indices]
        <span style="color:#66d9ef">return</span> (prediction_answer_ids <span style="color:#f92672">==</span> target_answer_ids<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">1</span>))<span style="color:#f92672">.</span>any(<span style="color:#ae81ff">1</span>)
</code></pre></div><blockquote>
<p>To compute top k average precision score (from Raffle&rsquo;s codebase)</p>
</blockquote>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">batch_map</span>(relevance, k):
    top_k <span style="color:#f92672">=</span> relevance[:, :k]
    batch_size <span style="color:#f92672">=</span> top_k<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">0</span>)
    pos <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">1</span>, top_k<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">1</span>) <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>, dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>float)<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">0</span>)<span style="color:#f92672">.</span>repeat(batch_size, <span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>to(top_k<span style="color:#f92672">.</span>device)
    csum, num_ans <span style="color:#f92672">=</span> top_k<span style="color:#f92672">.</span>cumsum(<span style="color:#ae81ff">1</span>), top_k<span style="color:#f92672">.</span>sum(<span style="color:#ae81ff">1</span>)
    apk <span style="color:#f92672">=</span> ((csum <span style="color:#f92672">/</span> pos <span style="color:#f92672">*</span> top_k)<span style="color:#f92672">.</span>sum(<span style="color:#ae81ff">1</span>) <span style="color:#f92672">/</span> num_ans)
    apk <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>masked_select(apk, <span style="color:#f92672">~</span>torch<span style="color:#f92672">.</span>isnan(apk))
    <span style="color:#66d9ef">return</span>  apk<span style="color:#f92672">.</span>sum()
</code></pre></div><p>We log 3 types of metrics: loss, accuracy and average precision (MAP).</p>
<aside class="notice">
When we say top *k* we measure a success criteria as each instance where the correct answer id lies in the *k* top scores. Whereas accuracy is blind to how high the relative score was, as long as it still falls in the top k, MAP will still (slightly) penalize correct answers that were not ranked highly enough. 
</aside>
<ul>
<li>Loss
<ul>
<li>Validation Loss</li>
<li>Train loss</li>
</ul>
</li>
<li>Accuracy
<ul>
<li>Questions
<ul>
<li>k = 1, k = 3, k = 10</li>
</ul>
</li>
<li>Conversations
<ul>
<li>k = 1, k = 3, k = 10</li>
</ul>
</li>
<li>Overall
<ul>
<li>k = 1, k = 3, k = 10</li>
</ul>
</li>
</ul>
</li>
<li>MAP
<ul>
<li>Questions
<ul>
<li>k = 3, k = 10</li>
</ul>
</li>
<li>Conversations
<ul>
<li>k = 3, k = 10</li>
</ul>
</li>
<li>Overall
<ul>
<li>k = 3, k = 10</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>As our project aims to increase the general performance of the model, this means that we want to identify if one query type is significantly worse than the other. Thus, it was important for us to track how our changes to the model affected each of the query types. Moreover, we specifically looked at the accuracy top 3 metrics to compare with Raffle&rsquo;s own model. Lastly, we used the loss metrics to monitor for signs of overfitting.</p>

  
    



  



<h1 id="timeline">Timeline</h1><p>This section will be a general summary of our work and the evolution of the project. The project went through 3 overall phases in regards to the construction of the model.</p>
<ol>
<li>Exploratory Data Analysis (EDA)</li>
<li>Document Ranking Model</li>
<li>Barlow Twins</li>
</ol>




  



<h2 id="exploratory-data-analysis">Exploratory Data Analysis</h2><p>The input data contains 4 main pieces of information. The title, the content, the type (questions or conversations) and answer id (the &ldquo;true&rdquo; label). Each piece of information should be analysed due to its importance. Grouping title and content together, in a group that can be called text, we end up with 3 types of information that must be analyzed. The answer ids can tell us something about the grouping of information, the question type can tell the balance of a dataset and the text can show whether questions and conversations are seperated.</p>




  



<h3 id="answer-ids">Answer ids</h3><p>With some qualitative analysis, we investigated the assumption that input data which points to the same answer is about the same. This assumption is hard to fully investigate, but the crude investigated showed that the assumption is not completely off. However, to say that the assumption is true would be wrong. Based on this, we choose to apply the assumption, since it reduces the complexity of the problem and allows us to group the input data more easily.</p>




  



<h3 id="dataset-imbalance">Dataset imbalance</h3><p>The datasets (train and validation) are heavily skewed, having about 10 times the amount of conversations as opposed to questions. By including the generated questions, we get to a 2/3 split of conversations and questions, giving a more balanced dataset. Furthermore, we investigated casting short conversations as questions, as some conversations can be as short as a single sentence, esssentially making them a question. We chose to cast conversations with less than 100 characters into questions, making an almost perfectly balanced dataset. The balanced dataset is also crucial for the Barlow implementation later.</p>




  



<h3 id="pca">PCA</h3><p>In order to investigate whether questions and conversations, that has the same answer id, lie closely to eachother in latent space, we applied PCA to reduce the dimensionality down to 2D in order to visualize it. There was two main takeaways from the PCA plots. Firstly, there did not appear to be distinct groupings based on answer ids and the embeddings was scattered in the latent space. There were certain areas with higher density of the same answer ids, but nothing that indicated seperation. Secondly, questions and conversations were not seperated and laid ontop of eachother. Howewer, conversatons were spread over the entire latent space, but questions appeared to be more clustered in certain areas. This might mean that BERT somewhat distinguishes between conversations and questions at a certain level.</p>




  



<h2 id="document-ranking-model">Document Ranking Model</h2><p>The first part of the project was to implement and understand a BERT model trained for Document Ranking.</p>




  



<h3 id="preprocessing-for-bert">Preprocessing for BERT</h3><blockquote>
<p>Step 1. Compute the TF-IDF score for each word in the corpus</p>
</blockquote>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">TFIDF_standard</span>(data):
	title <span style="color:#f92672">=</span> [x[<span style="color:#e6db74">&#34;title&#34;</span>] <span style="color:#66d9ef">for</span> x <span style="color:#f92672">in</span> data]
	content <span style="color:#f92672">=</span> [x[<span style="color:#e6db74">&#34;content&#34;</span>] <span style="color:#66d9ef">for</span> x <span style="color:#f92672">in</span> data]
	vectorizer <span style="color:#f92672">=</span> TfidfVectorizer(token_pattern<span style="color:#f92672">=</span><span style="color:#e6db74">r</span><span style="color:#e6db74">&#39;(?u)\b\w+\b&#39;</span>)
	lookup <span style="color:#f92672">=</span> vectorizer<span style="color:#f92672">.</span>fit(content)<span style="color:#f92672">.</span>vocabulary_
    input_string, kept_indices <span style="color:#f92672">=</span> []
</code></pre></div><blockquote>
<p>Step 2. Compute the relative TF-IDF score for each sentence</p>
</blockquote>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">	word_split <span style="color:#f92672">=</span> re<span style="color:#f92672">.</span>compile(<span style="color:#e6db74">r</span><span style="color:#e6db74">&#39;(?u)\b\w+\b&#39;</span>)
	tokenizer <span style="color:#f92672">=</span> nltk<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>load(<span style="color:#e6db74">&#39;tokenizers/punkt/english.pickle&#39;</span>)
	<span style="color:#66d9ef">for</span> i, document <span style="color:#f92672">in</span> enumerate(content):
		sentence_scores <span style="color:#f92672">=</span> []
		<span style="color:#66d9ef">for</span> sentence <span style="color:#f92672">in</span> tokenizer<span style="color:#f92672">.</span>tokenize(document):
			score <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
			<span style="color:#66d9ef">for</span> word <span style="color:#f92672">in</span> word_split<span style="color:#f92672">.</span>findall(sentence<span style="color:#f92672">.</span>lower())
				score <span style="color:#f92672">+=</span> lookup[word]
			num_words <span style="color:#f92672">=</span> max(<span style="color:#ae81ff">1</span>, len(words))
			sentence_scores<span style="color:#f92672">.</span>append(score<span style="color:#f92672">/</span>num_words) 
</code></pre></div><blockquote>
<p>Step 3. Order each sentence from most to least important based on relative TF-IDF score</p>
</blockquote>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">		sentence_scores <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(sentence_scores)
		sorted_indices <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>argsort(sentence_scores)
</code></pre></div><blockquote>
<p>Step 4. Take the max number of sentence (from most important to least) you can fit in 128 tokens</p>
</blockquote>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">        current_length <span style="color:#f92672">=</span> len(word_split<span style="color:#f92672">.</span>findall(title[i])) <span style="color:#75715e"># title is always returned</span>
		<span style="color:#75715e"># Greedy Knapsack solution to add sentences per average char weight</span>
		<span style="color:#66d9ef">for</span> index <span style="color:#f92672">in</span> sorted_indices:
			word_count <span style="color:#f92672">=</span> word_split<span style="color:#f92672">.</span>findall(sentences[index])
			<span style="color:#66d9ef">if</span> current_length <span style="color:#f92672">&lt;</span> TFIDF_answer_length:
				kept_indices<span style="color:#f92672">.</span>append(index)
				current_length <span style="color:#f92672">+=</span> len(word_count)
</code></pre></div><blockquote>
<p>Step 5. Re-order the sentences to the original order</p>
</blockquote>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">		final_string <span style="color:#f92672">=</span> title[i] <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34;.&#34;</span>
		<span style="color:#66d9ef">for</span> index <span style="color:#f92672">in</span> sorted(kept_indices):
			final_string <span style="color:#f92672">+=</span> <span style="color:#e6db74">&#34; &#34;</span> <span style="color:#f92672">+</span> sentences[index]
		input_string<span style="color:#f92672">.</span>append(final_string)
		
	<span style="color:#66d9ef">return</span>  input_string
</code></pre></div><p>As the input to BERT has to be of limited size due to time and memory constraints, we preprocess the input, to remove noise in the data and feed BERT with only the most useful information.</p>
<p>To do so, we decided to include the sentences with the highest importance according to relative sentence TF-IDF score. We computed this in the following way:</p>
<ol>
<li>Compute the TF-IDF score for each word in the corpus</li>
<li>Compute the relative TF-IDF score for each sentence
<ul>
<li>First sum up the total TF-IDF score for each word in the sentence</li>
<li>Divide by the amount of words in the sentence to compute a relative score</li>
</ul>
</li>
<li>Order each sentence from most to least important based on relative TF-IDF score</li>
<li>Take the max number of sentence (from most important to least) you can fit in 128 tokens</li>
<li>Re-order the sentences to the original order</li>
</ol>
<p>The entire code is demonstrated on the right.</p>
<p>Additionally, we tried utilizing the sections.json file, and instead return an equal number of top TF-IDF scored sentences <em>per section</em>, however this implementation did not yield any significant improvements. In results, we will show the performance of this version of TF-IDF in comparison to the vanilla version.</p>
<p>Moreover, we also attempted to increase the amount of tokens from 128 to 258 and 512; this did not yield any significant improvements. Instead it merely increased runtime lineraly in proportion to the number of tokens. (Twice the amount of data took roughly twice the amount of time). This version of TF-IDF will also be discussed in the results.</p>




  



<h3 id="bert-with-one-head">BERT with one head</h3><aside class="notice">
For clarity, question and conversations embeddings will be called "queries".
</aside>
<p>The model was implemented as follows:</p>
<ol>
<li>Compute the answer Database
<ul>
<li>We passed the answers through a BERT model with a head to get a database of answer encodings.</li>
</ul>
</li>
<li>Compute the query embeddings
<ul>
<li>Next, we passed the questions and conversations through the same BERT model with the same head to get their encodings.</li>
</ul>
</li>
</ol>
<p><img src="https://nklingen.github.io/DocRankingWebsite//images/BERT1.png" alt="BERT 1"></p>
<ol start="3">
<li>Document Ranking
<ul>
<li>To compute most related documents for a given query, we compute the cross product between the query embeddings and the entire answer database.</li>
<li>We then find the argmax for each query, that is the answer document that had the highest score for that query.</li>
<li>We return the index of the argmax.</li>
</ul>
</li>
</ol>
<p><img src="https://nklingen.github.io/DocRankingWebsite//images/BERT2.png" alt="BERT 2"></p>
<p>This is a simplified explanation for the instance where k=1. To compute the top k most relevant documents, the indices of the top k argmax elements are returned for each query.</p>
<p>We will also discuss some design choices in implementing BERT. Namely, the two key decisions in this phase were <strong>freezing BERT</strong> and <strong>only using the CLS tokens</strong>.</p>
<ul>
<li>The decision to freeze BERT parameters was primarily taken to increase speed and avoid running into issues of memory constraint, which was crutial for us given this was a short-term research project and we had GPU resource constraints. We knew this would entail a decrease in accuracy. Unfreezing BERT and adapting the model for all experiments would have likely increased performance.</li>
<li>The second decision was using only the CLS tokens. Briefly, the CLS token is first column of the BERT output, typically used for classification tasks. The CLS token can be said to encapsulate all the information of the input, to give a sort of general summary. We deemed this token most important for ranking. Thus, given the same speed and memory constraints, we chose to only use the CLS token and discard the rest of the BERT output. However, as we will discuss later, this can also come with consequences.</li>
</ul>
<p>We apply some of the common deep learning improvements, such as gradient clipping, and l2 regularization, and saw immediate improvements. These techniques help us stabilize the learning and prevent overfitting.</p>
<p>Later, we also exchanged the gradient clipping after the model terminates with batch normalization in between the blocks of the model.</p>
<p>Having tried variations of pre-processing and manually updates to the model, we then decided to to fix a set of parameters for our model for all following experiments and thus &ldquo;freeze&rdquo; our model. We tune with respect to the validation loss over 4 parameters: dropout rate, learning rate, batch size and weight decay. We utilized &lsquo;Weights &amp; Biases&rsquo; automatic hyperparameter tuning (sweeps) and chose to use Bayes optimisation to optimise the search.</p>
<p>As the graphics are a bit small, the entire Hyperparameter Tuning Sweep can be found <a href="https://wandb.ai/nklingen/BERT_Question_Answering/reports/Hyperparameter-Tuning-Baseline--VmlldzoxMTQ0OTE5">here</a>.</p>
<p><img src="https://nklingen.github.io/DocRankingWebsite//images/sweep.jpg" alt="Overview of all the runs"></p>
<p>We fix the <code>max_length=128</code>, i.e. the amount of tokens fed to BERT, and the max number of <code>epochs=50</code>. The sweep finds the following parameters:</p>
<p><code>batch size=50</code>
<code>dropout=0.11</code>
<code>learning rate=1.4e-4</code>
<code>weight decay=9.3e-4</code></p>
<p>Moreover, &lsquo;Weights &amp; Biases&rsquo; tells us the importance and correlation of each parameter. It clearly shows the learning rate being the most influential, which is expected due to its large impact on training.</p>
<p><img src="https://nklingen.github.io/DocRankingWebsite//images/param_importance.png" alt="Parameter importance"></p>




  



<h3 id="bert-with-dual-encoder">BERT with Dual Encoder</h3><blockquote>
<p>Using the Dual encoder, we want to set both models to train</p>
</blockquote>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span>  <span style="color:#a6e22e">train</span>(model, model_answers, optimizer, criterion, data_loader, answer_loader, cls_dictionary):

    answer_tokens <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>stack(list(cls_dictionary<span style="color:#f92672">.</span>values()))<span style="color:#f92672">.</span>to(device)
    answer_ids <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(list(cls_dictionary<span style="color:#f92672">.</span>keys()))<span style="color:#f92672">.</span>to(device)
    total_loss <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
    model<span style="color:#f92672">.</span>train() <span style="color:#75715e"># Query encodings</span>
    model_answers<span style="color:#f92672">.</span>train() <span style="color:#75715e"># Answer DB encodings</span>
</code></pre></div><blockquote>
<p>For each new batch from the queries, we first update the stale index over a batch of answers</p>
</blockquote>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">    answer_iterator <span style="color:#f92672">=</span> iter(answer_loader)
    <span style="color:#66d9ef">for</span>  step, (data_batch) <span style="color:#f92672">in</span>  enumerate(data_loader):
        answer_batch <span style="color:#f92672">=</span> next(answer_iterator)
        optimizer<span style="color:#f92672">.</span>zero_grad()
        answer_encoding, _, attention_mask, batch_ids <span style="color:#f92672">=</span> answer_batch
        new_answer_tokens  <span style="color:#f92672">=</span> model_answers(answer_encoding, attention_mask)
        answer_tokens[(answer_ids <span style="color:#f92672">==</span> batch_ids<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">1</span>))<span style="color:#f92672">.</span>nonzero()[:,<span style="color:#ae81ff">1</span>]] <span style="color:#f92672">=</span> new_answer_tokens
</code></pre></div><blockquote>
<p>We then pass the queries though their respective model to compute their encoding</p>
</blockquote>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">        input_encoding, _, attention_mask, target_answer_ids, question_type <span style="color:#f92672">=</span> data_batch
        question_tokens <span style="color:#f92672">=</span> model(input_encoding, attention_mask)
</code></pre></div><blockquote>
<p>Finally we compute the score between the encodings and the answer database</p>
</blockquote>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">        scores <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>matmul(question_tokens, answer_tokens<span style="color:#f92672">.</span>T)
        target_answer_indices <span style="color:#f92672">=</span> (answer_ids <span style="color:#f92672">==</span> target_answer_ids<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">1</span>))<span style="color:#f92672">.</span>nonzero()[:, <span style="color:#ae81ff">1</span>]
        loss <span style="color:#f92672">=</span> criterion(scores, target_answer_indices) <span style="color:#75715e"># compute loss </span>
        loss<span style="color:#f92672">.</span>backward() <span style="color:#75715e"># compute gradients</span>
        torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>utils<span style="color:#f92672">.</span>clip_grad_norm_(model<span style="color:#f92672">.</span>parameters(), <span style="color:#ae81ff">1</span>) <span style="color:#75715e"># gradient clipping </span>
        optimizer<span style="color:#f92672">.</span>step() <span style="color:#75715e"># update weights</span>
        answer_tokens<span style="color:#f92672">.</span>detach_()
        total_loss <span style="color:#f92672">+=</span> loss<span style="color:#f92672">.</span>item()
</code></pre></div><p>On a tip from Raffle, we next implemented a Dual Encoder, wherein the create two heads - one for the query data and one for the answers. Thus the two inputs are no longer encoded in the same model, and each encoder can specialise on its respective input.</p>
<p>During a training step the model will update part of the stale index, as seen in the code to the right. This updated stale index is used to score the given document for new incoming queries. In such, we collect gradients from both the indexing of answers and the scoring of questions and conversations (but later detach the model for the answers). We only calculate a single loss based on the scoring, and then backpropagate this loss onto the two seperate heads.</p>
<p><img src="https://nklingen.github.io/DocRankingWebsite//images/BERT3.png" alt="BERT 3"></p>




  



<h2 id="pretraining-with-barlow-twins">Pretraining with Barlow Twins</h2><p>At this stage, we had a very basic, working Document Ranking Model. We now wanted to try out our hypothesis, that pushing together similar question and conversation encodings might improve the model.</p>




  



<h3 id="theory">Theory</h3><p>Barlow Twins takes a batch of samples, applies noise to generate two distored versions, then passes both versions through two identical networks to get their corresponding embeddings. The Barlow loss is then computed on the embeddings, wherein the goal is to get the cross-correlation matrix between the embeddings as close as possible to the identity matrix. In this way, the embeddings of the two versions of the sample are encourraged to be similar, while redundancy between the components of the vectors is penalized.</p>
<p><em>&ldquo;Barlow Twins is competitive with state-of-the-art methods for self-supervised learning while being conceptually simpler, naturally avoiding trivial constant (i.e. collapsed) embeddings, and being robust to the training batch size.&quot;</em> From: <a href="https://arxiv.org/pdf/2103.03230.pdf">Barlow Twins: Self-Supervised Learning via Redundancy Reduction</a></p>
<p>In our project, we implement Barlow Twins to encourage similar embeddings between questions and conversations with the same <code>answer_id</code>. The assumption here is that questions and conversations that have the same answer must be related in some capacity, and moreover, that they must share an underlying concept. Hereby, the two &ldquo;versions&rdquo; can be likened to distortion of the same underlying concept.</p>
<p>Whereas the original Barlow Paper takes a batch of samples and applies noise, in our implementation we already have the distorted matrices. For a given <code>answer_id</code>, this is simply a batch with associated questions (f), and a batch with associated conversations (g). In our dataloader, we ensure the batches are of equal size.</p>
<p><img src="https://nklingen.github.io/DocRankingWebsite//images/Barlow_1.png" alt="Barlow 1"></p>
<p>Now we compute the correlation between f and g.
<img src="https://nklingen.github.io/DocRankingWebsite//images/Barlow_eq.png" alt="Barlow EQ"></p>
<p>This yields the following correlation matrix:
<img src="https://nklingen.github.io/DocRankingWebsite//images/Barlow_matrix.png" alt="Barlow Matrix"></p>
<p>Having now computed the correlation matrix, we want to encourage it to resemble the identity matrix. Hereby, we have two terms. In the <code>invariance_term</code> we encourage the diagonals (marked in grey) to be close to 1 and hereby for the model to be distortion agnostic, while in the <code>redundancy_reduction_term</code> we encourage all off-diagonals to be close to 0.</p>
<p><img src="https://nklingen.github.io/DocRankingWebsite//images/Barlow_3.png" alt="Barlow 3"></p>




  



<h3 id="code">Code</h3><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">
<span style="color:#66d9ef">for</span> batch <span style="color:#f92672">in</span> B:
    <span style="color:#f92672">...</span> 
    input_encoding_ques, input_encoding_conv, _, _, attention_mask_conv, attention_mask_ques <span style="color:#f92672">=</span> batch
    barlow_sample_batch_size <span style="color:#f92672">=</span> input_encoding_ques<span style="color:#f92672">.</span>squeeze()<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]

    <span style="color:#75715e"># question (f) and conversation (g) encodings</span>
    f <span style="color:#f92672">=</span> model(input_encoding_ques<span style="color:#f92672">.</span>squeeze(), attention_mask_ques<span style="color:#f92672">.</span>squeeze()) 
    g <span style="color:#f92672">=</span> model(input_encoding_conv<span style="color:#f92672">.</span>squeeze(), attention_mask_conv<span style="color:#f92672">.</span>squeeze())  

    <span style="color:#75715e"># normalize along the batch dimensions, thus we have the normalized features across all batches</span>
    f_norm <span style="color:#f92672">=</span> (f <span style="color:#f92672">-</span> f<span style="color:#f92672">.</span>mean(<span style="color:#ae81ff">0</span>)) <span style="color:#f92672">/</span> f<span style="color:#f92672">.</span>std(<span style="color:#ae81ff">0</span>)
    g_norm <span style="color:#f92672">=</span> (g <span style="color:#f92672">-</span> g<span style="color:#f92672">.</span>mean(<span style="color:#ae81ff">0</span>)) <span style="color:#f92672">/</span> g<span style="color:#f92672">.</span>std(<span style="color:#ae81ff">0</span>)

    <span style="color:#75715e"># cross-correlation matrix</span>
    c <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>matmul(f_norm<span style="color:#f92672">.</span>T, g_norm)<span style="color:#f92672">/</span> barlow_sample_batch_size

    <span style="color:#75715e"># Barlow Loss</span>
    invariance_term <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>diagonal(c)<span style="color:#f92672">.</span>add_(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>pow_(<span style="color:#ae81ff">2</span>)<span style="color:#f92672">.</span>sum()
    redundancy_reduction_term <span style="color:#f92672">=</span> off_diagonal(c)<span style="color:#f92672">.</span>pow_(<span style="color:#ae81ff">2</span>)<span style="color:#f92672">.</span>sum()
    loss <span style="color:#f92672">=</span> invariance_term <span style="color:#f92672">+</span> lambd <span style="color:#f92672">*</span> redundancy_reduction_term
    <span style="color:#f92672">...</span>
</code></pre></div><p>Now to discuss our implementation. We create a specific dataloader for Barlow that passes through a batch of questions and a batch of conversations with the same <code>answer_id</code>, of equal size. Both batches are passed through the same model (a frozen BERT &amp; a trainable barlow head)</p>
<p>The the cross-correlation matrix c is computed as discussed previously, and finally normalized with the corresponding batch size.</p>
<p><code>c = (f - mean(f))*(g - mean(g))/(std(f) * std(g))/batch_size</code>,</p>
<p>The reason for normalising the cross-correlation matrix is because the batch_size is not constant between new <code>answer_ids</code>. Some <code>answer_ids</code> have only a few questions and conversations, where as others may have hundreds. Moreover, the two classes may be imbalanced for any given <code>answer_id</code>. To solve this, and to maximize the batch sizes, the data_loader first computes the maximal possible batch size for the <code>answer_id</code>.</p>
<p><code>batch_size = min(len(questions), len(conversations))</code></p>
<p>Then it takes all datapoints from the smaller set, and samples from the larger set until we have an equal number of data points from both sets.</p>
<p>Lastly, the <code>invariance_term</code> and <code>redundancy_term</code> are computed using two helper functions, <code>diagonal</code> and <code>off_diagonal</code> that return flattened versions of all elements in the diagonal or off-diagonal. The two terms are implemented exactly as described in <a href="#theory">Theory</a></p>




  



<h3 id="implementation">Implementation</h3><p><strong>Pre-training with inherited head</strong></p>
<p>We initially pretrainined with a Barlow Head and then passed the head direction to the Dual-Encoder Document Ranking Model. We found that while we saw that the shared head was converging very smoothly for the pre-training, and showed excellent results in pushing together the embeddings, it had very poor one-shot performance in Document Ranking, and did not converge more quickly.</p>
<p><strong>Pre-training with frozen head</strong></p>
<p>Consequently, we hypothesized that the model was not sufficiently complex to capture both the Barlow objective (push together question and conversation embeddings) and the Ranking objective (score highly on document ranking). That is, after training for the Barlow objective, the weights in the head were quickly overwritten when it was tasked with optimising for the Ranking objective. To counteract this (and thus get a true evaluation of applying Barlow Twins as a pretraining step), we decided to freeze the Barlow Twins head during training. Thus, the model is forced to rank documents <em>without unlearning</em> the embeddings from the pretraining.</p>
<p>The architecture for Pre-training with frozen head is shown below.</p>

  
    



  



<h1 id="model">Model</h1><blockquote>
<p>The two models are outlined below, where <a style="color:tomato">red</a> indicates sub-models with frozen parameters and where <a style="color:dodgerblue">blue</a> indicates sub-models with trainable parameters.</p>
</blockquote>
<blockquote>
<p><strong>Pre-training Model</strong></p>
<ol>
<li>
<p style="color:tomato"> BERT</p>
</li>
<li>
<p style="color:dodgerblue"> Barlow Head</p>
</li>
</ol>
</blockquote>
<blockquote>
<p><strong>Training Model</strong></p>
<ol>
<li>
<p style="color:tomato"> Pre-training Model</p>
<p style="color:tomato">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Bert<p>
<p style="color:tomato">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Barlow Head</p>
</li>
<li>
<p style="color:dodgerblue"> Ranking Head</p>
</li>
</ol>
</blockquote>
<p>The following section will discuss how the pre-training (with frozen head) and training models are constructed.</p>




  



<h2 id="bert-base">BERT Base</h2><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">BERT</span>(nn<span style="color:#f92672">.</span>Module):
    <span style="color:#66d9ef">def</span> __init__(self, config):

        super(BERT, self)<span style="color:#f92672">.</span>__init__()
        <span style="color:#75715e"># Reference: https://huggingface.co/Maltehb/danish-bert-botxo/blob/main/README.md</span>
        self<span style="color:#f92672">.</span>bert <span style="color:#f92672">=</span> AutoModel<span style="color:#f92672">.</span>from_pretrained(<span style="color:#e6db74">&#34;Maltehb/danish-bert-botxo&#34;</span>)

        <span style="color:#75715e"># freeze all the parameters in BERT. This prevents updating of model weights during fine-tuning</span>
        <span style="color:#66d9ef">for</span> param <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>bert<span style="color:#f92672">.</span>parameters():
            param<span style="color:#f92672">.</span>requires_grad <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, input_id, mask):
        BERT_output <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>bert(input_id, attention_mask<span style="color:#f92672">=</span>mask)
        <span style="color:#66d9ef">return</span> BERT_output
</code></pre></div><p>To delve further into the code, the BERT base simply passes the <code>input_id</code> and <code>attention_masks</code> from the Bert tokeniser through the Danish BERT model, downloaded from <a href="https://huggingface.co/Maltehb/danish-bert-botxo/blob/main/README.md">Hugging Face</a>.</p>
<p>We made an initial choice to only train the head (for time and complexity reasons), therefore in <code>init</code> we freeze all BERT paramters so they will never be updated.</p>




  



<h2 id="barlow-head">Barlow Head</h2><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">barlow_HEAD</span>(nn<span style="color:#f92672">.</span>Module):

    <span style="color:#66d9ef">def</span> __init__(self, config):
        super(barlow_HEAD, self)<span style="color:#f92672">.</span>__init__()
        self<span style="color:#f92672">.</span>dropout <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Dropout(config<span style="color:#f92672">.</span>dropout)
        self<span style="color:#f92672">.</span>relu <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>ReLU()
        self<span style="color:#f92672">.</span>fc1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">768</span>, <span style="color:#ae81ff">768</span>)
        self<span style="color:#f92672">.</span>fc2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">768</span>, <span style="color:#ae81ff">768</span>)
        self<span style="color:#f92672">.</span>bn1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>BatchNorm1d(<span style="color:#ae81ff">768</span>)
        self<span style="color:#f92672">.</span>bn2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>BatchNorm1d(<span style="color:#ae81ff">768</span>)

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, BERT_output):
        cls <span style="color:#f92672">=</span> BERT_output<span style="color:#f92672">.</span>last_hidden_state[:, <span style="color:#ae81ff">0</span>, :]
        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>bn1(self<span style="color:#f92672">.</span>fc1(cls))
        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>relu(x)
        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>dropout(x)
        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>bn2(self<span style="color:#f92672">.</span>fc2(x))
        <span style="color:#66d9ef">return</span> x
</code></pre></div><p>The goal of this model is to push together question and conversation encodings.</p>
<p>We apply the following steps:</p>
<ol>
<li>capture the CLS token from the BERT output.</li>
<li>Pass through a fully connected layer with batch normalisation</li>
<li>Apply a ReLU activation function</li>
<li>Apply Dropout</li>
<li>Pass through a second fully connected layer with batch normalisation</li>
</ol>




  



<h2 id="ranking-head">Ranking Head</h2><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">ranking_HEAD</span>(nn<span style="color:#f92672">.</span>Module):

    <span style="color:#66d9ef">def</span> __init__(self, config):
        super(ranking_HEAD, self)<span style="color:#f92672">.</span>__init__()
        self<span style="color:#f92672">.</span>dropout <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Dropout(config<span style="color:#f92672">.</span>dropout)
        self<span style="color:#f92672">.</span>relu <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>ReLU()
        self<span style="color:#f92672">.</span>fc1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">768</span>, <span style="color:#ae81ff">768</span>)
        self<span style="color:#f92672">.</span>fc2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">768</span>, <span style="color:#ae81ff">768</span>)
        self<span style="color:#f92672">.</span>bn1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>BatchNorm1d(<span style="color:#ae81ff">768</span>)
        self<span style="color:#f92672">.</span>bn2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>BatchNorm1d(<span style="color:#ae81ff">768</span>)

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>bn1(self<span style="color:#f92672">.</span>fc1(x))
        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>relu(x)
        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>dropout(x)
        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>bn2(self<span style="color:#f92672">.</span>fc2(x))
        <span style="color:#66d9ef">return</span> x

</code></pre></div><p>The goal of this model is to pair question and conversation encodings with document encodings to optimise the Document Ranking score.</p>
<p>We apply the following steps:</p>
<ol>
<li>Pass through a fully connected layer with batch normalisation</li>
<li>Apply a ReLU activation function</li>
<li>Apply Dropout</li>
<li>Pass through a fully connected layer with batch normalisation</li>
</ol>




  



<h2 id="pre-training-model">Pre-training Model</h2><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">pretrainingModel</span>(nn<span style="color:#f92672">.</span>Module):

    <span style="color:#66d9ef">def</span> __init__(self, config):
        super(pretrainingModel, self)<span style="color:#f92672">.</span>__init__()
        self<span style="color:#f92672">.</span>bert <span style="color:#f92672">=</span> BERT(config)
        self<span style="color:#f92672">.</span>barlow_HEAD <span style="color:#f92672">=</span> barlow_HEAD(config)

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, input_id, mask):
        BERT_output <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>bert(input_id, mask)
        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>barlow_HEAD(BERT_output)
        <span style="color:#66d9ef">return</span> x
</code></pre></div><p>The pre-training model simply passes the <code>input_id</code> and <code>mask</code> from the tokeniser through the bert model and the barlow head. The BERT parameters are frozen in the <a href="#bert-base">Bert Base</a> <code>init</code>, so only the <a href="#barlow-head">Barlow Head</a> parameters train.</p>




  



<h2 id="training-model">Training Model</h2><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">trainingModel</span>(nn<span style="color:#f92672">.</span>Module):

    <span style="color:#66d9ef">def</span> __init__(self, pretrainingModel: pretrainingModel, config):
        super(trainingModel, self)<span style="color:#f92672">.</span>__init__()

        <span style="color:#75715e"># inherit the BERT and BarlowHEAD from pretraining model</span>
        self<span style="color:#f92672">.</span>pretrainingModel <span style="color:#f92672">=</span> pretrainingModel
        <span style="color:#75715e"># freeze all the parameters in barlow_HEAD. </span>
        <span style="color:#75715e"># This embedding head shouldn&#39;t learn a new task. </span>
        <span style="color:#75715e"># Ranking head needs to adapt to embedding provided.</span>
        <span style="color:#66d9ef">for</span> param <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>pretrainingModel<span style="color:#f92672">.</span>parameters():
            param<span style="color:#f92672">.</span>requires_grad <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>
            
        self<span style="color:#f92672">.</span>ranking_HEAD <span style="color:#f92672">=</span> ranking_HEAD(config)
    
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">set_pretrain_model_to_eval</span>(self):
        self<span style="color:#f92672">.</span>pretrainingModel<span style="color:#f92672">.</span>eval()
    
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, input_id, mask):
        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>pretrainingModel(input_id, mask)
        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>ranking_HEAD(x)
        <span style="color:#66d9ef">return</span> x
</code></pre></div><p>In the Training-Model, the <a href="#pre-training-model">pre-training model</a> (including both <a href="#bert-base">Bert Base</a> and <a href="#barlow-head">Barlow Head</a>) is passed as a parameter. All model parameters are frozen, such that the training model will not update parameters from the pre-training model. During a forward pass, input is passed through the frozen pre-training model and then through an addition <a href="#ranking-head">Ranking Head</a> which is trained with the objective of optimising document ranking scores.</p>
<p>Moreover, the training-model includes a method <code>set_pretrain_model_to_eval</code> that is called from the main script. When the Training Model is set to train mode, we want to ensure that only the ranking head is actually in train mode, while all frozen pre-training sub-models remain in eval mode. Thus, <code>set_pretrain_model_to_eval</code> sets the <a href="#pre-training-model">pre-training model</a> back to eval.</p>

  
    



  



<h1 id="results">Results</h1><p>Each model is set to terminate after 10 epochs without improvement of the validation loss. This is what causes the different termination times in the below plots.</p>




  



<h3 id="document-ranking-results">Document Ranking Results</h3><p>We begin with the baseline model, otherwise known as BERT with one head. We compare it with plots from our pre-processing experiments: TF-IDF pre-processing by section, or with 512 tokens. These results show us that the optimal pre-processing was just the initial implementation. Next, we implemented the Dual Encoder model, which we see gave a significant performance boost. We compare the three models via validation loss, Accuracy, and MAP.</p>
<blockquote>
<p>The best model is by far the dual encoder implementation. This implementation had a more gradual learning curve, starting off worse than the other single-head models, but was also to train for longer (10-20 epochs before stagnating) and giving significant improvements on all metrics. Most noticably, the dual encoder implementation surpasses the other models before they stagnate. This means that the dual encoder&rsquo;s increased performance is not just due to the longer training. Stopping the dual encoder model at the same point as the others would have also yielded significant improvements.</p>
</blockquote>
<p><img src="https://nklingen.github.io/DocRankingWebsite//images/validation_loss.png" alt="Validation loss of the generic improvements"></p>
<table>
<thead>
<tr>
<th>Model / Top 3 accuracy</th>
<th>Total</th>
<th>Question</th>
<th>Conversation</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Baseline</td>
<td>0.44</td>
<td>0.32</td>
<td>0.50</td>
<td></td>
</tr>
<tr>
<td>Sections</td>
<td>0.43</td>
<td>0.29</td>
<td>0.49</td>
<td></td>
</tr>
<tr>
<td>Dual encoder</td>
<td>0.49</td>
<td>0.34</td>
<td>0.57</td>
<td></td>
</tr>
<tr>
<td>Baseline 512 tokens</td>
<td>0.43</td>
<td>0.32</td>
<td>0.49</td>
<td></td>
</tr>
</tbody>
</table>
<blockquote>
<p>We see that the model is significantly better at ranking conversations than questions, which is quite unexpected. Conversations are usually more noisy and harder to interpret than questions. However, conversations also contain additional information that the model may have benefitted from.</p>
</blockquote>
<table>
<thead>
<tr>
<th>Model / Top 3 MAP</th>
<th>Total</th>
<th>Question</th>
<th>Conversation</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Baseline</td>
<td>0.36</td>
<td>0.24</td>
<td>0.42</td>
<td></td>
</tr>
<tr>
<td>Sections</td>
<td>0.33</td>
<td>0.23</td>
<td>0.41</td>
<td></td>
</tr>
<tr>
<td>Dual encoder</td>
<td>0.40</td>
<td>0.26</td>
<td>0.47</td>
<td></td>
</tr>
<tr>
<td>Baseline 512 tokens</td>
<td>0.34</td>
<td>0.23</td>
<td>0.40</td>
<td></td>
</tr>
</tbody>
</table>




  



<h3 id="barlow-results">Barlow Results</h3><blockquote>
<p>There is a clear and smooth loss curve. This is particularly promising as, due to the sampling in each new batch, the model will rarely encounter the same data set twice (particularly if there is a large imbalance between the classes).</p>
</blockquote>
<p><img src="https://nklingen.github.io/DocRankingWebsite//images/barlow_loss.png" alt="The loss during Barlow training"></p>




  



<h3 id="barlow-and-document-ranking">Barlow and Document Ranking</h3><p>We will now compare the three models.</p>
<ol>
<li><em>baseline</em> - the Baseline Dual Encoder</li>
<li><em>zero-shot</em> - the Barlow pre-training Model zero-shot on the training data</li>
<li><em>pre-trained baseline</em> - the Barlow pre-training model with frozen parameters followed by the Dual Encoder.</li>
</ol>
<p>The pretraining&rsquo;s goal was to push questions and conversations together. In order to evaluate this, we will discuss two metrics.</p>
<ul>
<li>First, we will qualitatively evaluate the differences in our PCA plots.</li>
<li>Secondly we will present a table of different distances mesaurements that will allow us to quantify the differences between the models.</li>
</ul>
<p>The PCA plots contain the question and conversation centroids for every answer id. Each PCA plot consists of 3 subplots, where each plot varies in the method of PCA.</p>
<ul>
<li><em>High dim</em> calculates the centroids in the high dimension embeddings before projecting down to 2D.</li>
<li><em>Med dim</em> reduces dimensionality to 30 before calculating the centroids then projects down to 2D.</li>
<li><em>low dim</em> reduces to 2 dimensions before calculating the centroids. This is done to reduce the influence of the dimensionality reduction so that proper conclusions can be drawn.</li>
</ul>
<blockquote>
<p>PCA for Baseline</p>
</blockquote>
<blockquote>
<p>Notice the clear seperation below of questions (1) and conversations (0).</p>
</blockquote>
<p><img src="https://nklingen.github.io/DocRankingWebsite//images/centroids_baseline.png" alt="PCA for baseline"></p>
<blockquote>
<p>PCA for Zeroshot</p>
</blockquote>
<blockquote>
<p>While the zeroshot fails document ranking, there is a clear improvement in encouraging closer embeddings in plot. The clear seperation has been removed, although we still see some signs of seperation.</p>
</blockquote>
<p><img src="https://nklingen.github.io/DocRankingWebsite//images/centroids_zeroshot.png" alt="PCA for zeroshot"></p>
<blockquote>
<p>PCA for Pre-trained Baseline</p>
</blockquote>
<blockquote>
<p>We see that the pretrained model is slightly more seperated when compared to the zeroshot, but is still better than the baseline. It appears that the model still perserves some of its pretraining after training.</p>
</blockquote>
<p><img src="https://nklingen.github.io/DocRankingWebsite//images/centroids_pretrained.png" alt="PCA for pretrained baseline"></p>
<p>From the above plots we clearly see that Barlow (and pretraining) accomplish the goals we want when looking at PCA-based metrics. However, we are more interested in the specific <code>answer id</code> groupings. Therefore, we create a table with the following two measurements to confirm the results above:</p>
<ul>
<li>a centroid distance, that is the average distance between the question and conversation centroid with the same <code>answer id</code></li>
<li>the average distance from every query to its corresponding answer.</li>
</ul>
<p>Both of these distances are calculated with the embeddings normalized in a range from 0 to 1.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Avg. dist centroid</th>
<th>Avg. dist to answer</th>
</tr>
</thead>
<tbody>
<tr>
<td>Baseline</td>
<td>2.09</td>
<td>0.059</td>
</tr>
<tr>
<td>Zero-shot</td>
<td>1.6</td>
<td>0.058</td>
</tr>
<tr>
<td>Pretrained Baseline</td>
<td>1.07</td>
<td>0.032</td>
</tr>
</tbody>
</table>
<aside class="notice">
The average distance centroid is a distance metric in multiple dimensions, which is why it may be above 1 even though the embeddings themselves have been normalized.
</aside>
<p>In comparison to the baseline&rsquo;s average distance to centroid:</p>
<ul>
<li>we expected the zero-shot model to have a <strong>significantly lower average distance to centroid</strong> since the Barlow Head is purely tasked with generating similar encodings for queries with the same answer id.</li>
<li>we expected the pre-trained model to have a <strong>lower average distance to centroid</strong>, given that potentially it might spread out some of the encodings in the training head to perform better on Document Ranking.</li>
</ul>
<p>From the PCA plots, the zero-shot model and the pretrained baseline that was allowed to train a new head for Document Ranking performed similarly. But in this table we see a clear descending progression in the average distance between centroids. There is about a 0.5 drop between each model. Furthermore, and the pretrained baseline is at approximately a 50% reduction from the baseline. This is counterintutitive because it seems that the model continued to push the embeddings together, even after the initial Barlow head was frozen and the new head was tasked purely with Document Ranking.</p>
<p>In comparison to the baseline&rsquo;s average distance to answer:</p>
<ul>
<li>we expected the zero-shot model to have a  <strong>slightly higher average distance to answer</strong>. While it never directly trains to push the embeddings closer to the answer_id, we expect the groups to move together to middle-cluster that would be closer to the answer on average.</li>
<li>we expected the pre-trained model to have a <strong>similar average distance to answer</strong>. Since it have the opportunity to train on the answers, we expected it to move closer to the answer than the zero-shot.</li>
</ul>
<p>We very oddly find that the average distance to answer is nearly the same as the baseline, without ever having seen the true answer. Moreover, the pretrained baseline also has a 50% reduction compared to the baseline. Overall, it looks like Barlow accomplishes its task of pushing both questions and conversation together, which in turn makes these closer to the answer they point to. In the discussion we will interpret this further.</p>
<p>From this we would expect that the zero-shot would have similar performance to the baseline, and that the pre-trained baseline would outperform the baselne. However, this was very far from the results we saw.</p>
<blockquote>
<p>We see that the pretrained baseline has completely collapsed at the start of training. The pretrained baseline, and the zero-shot at step 0, is totally unequipped for the document ranking task.  Essentially, on running different variations of the model, the starting point is  very poor. Moreover the model does not converge more quickly.</p>
</blockquote>
<p><img src="https://nklingen.github.io/DocRankingWebsite//images/pretraining.png" alt="Baseline vs. pretrained baseline"></p>
<p><img src="https://nklingen.github.io/DocRankingWebsite//images/TrainLoss.png" alt="Train Loss"></p>
<p>If we remove the zero-shot:</p>
<p><img src="https://nklingen.github.io/DocRankingWebsite//images/barlow_train_loss.png" alt="Barlow Train Loss"></p>
<p><img src="https://nklingen.github.io/DocRankingWebsite//images/barlow_val_loss.png" alt="Barlow Val Loss"></p>

  
    



  



<h1 id="discussion">Discussion</h1>



  



<h2 id="barlow">Barlow</h2><p>Our main challange for this project was to implement Barlow on a new domain, and for a new task. The Barlow Loss was originally used to train a model to be robust to noise. However, we implemented it instead to push embeddings closed together in a latent space. Moreover, we see that this implementation was indeed successful as we confirmed from the converging loss, decreasing distance to cluster-centroid, and visual PCA interpretation.</p>




  



<h2 id="barlow-with-document-ranking-bert">Barlow with Document Ranking BERT</h2><p>We created a simplified, working model of Raffle&rsquo;s Document Ranking BERT. However, as soon as we coupled our baseline with the Barlow Pre-training, the model performed significantly worse than the original baseline. In fact, the starting accuracy was nearly 0, giving the same performance as randomly guessing.</p>
<p>We tried with both frozen and unfrozen parameters being passed from the pretraining model to the training model, but in neither case could we see any improvement. We also hypothesized that this may be because the scaling is off, meaning that the embeddings are in an entirely different scale during pre-training than during training. This led us to implement batch-normalization in all the models to see if there was any improvement. However, the model did not improve.</p>
<p>When looking at the Barlow part of the results section, it looks quite peculiar. We see that Barlow trains nicely, but the model is completely destroyed when document ranking with respect to the pre-decided metrics. But then when we look at the PCA plots and the distance table, the results point to a success. Both the PCA plots show more promising embeddings without a linear seperation, and the distance table clearly shows that the queries are closer in the latent space, both to each other but also to their answer. This appears quite counterintuitive to our metric results.</p>
<p>We consider the possibility that the two models might just not be compatible. Essentially, maybe pushing the embeddings together is simply an inherently poor idea based on a faulty assumption that questions and conversations with the same <code>answer id</code> should lie in the same latent space. This might be a very human intuition that we are imposing on the model, which may not necessarily translate into computer understanding.</p>
<p>We also notice that the pretrained model is more or less broken for the ranking task. This hints to the fact that there might be a problem in the integration between the two models. Raffle.ai has had previous experience with BERT&rsquo;s apparent resistance to pretraining tasks. In other words, there might be a need for a more careful integration, rather than simply passing forward the trained BARLOW head put ontop of BERT which might interfere with BERT&rsquo;s understanding of language. Given our time constraints, these considerations were beyond the scope of this project. We propose this for the future work.</p>

  

      </div>
      <div class="dark-box">
        
          <div class="lang-selector" x-data="langController" x-init="initLangs([{&#34;key&#34;:&#34;python&#34;,&#34;name&#34;:&#34;Python&#34;}])">
            <template x-for="(tab, index) in tabs">
              <a x-text="tab.name" :class="{ 'active': tab.active }" @click="changeLanguage(index)"></a>
            </template>
          </div>
        
      </div>
    </div>
    
  </body>
</html>
