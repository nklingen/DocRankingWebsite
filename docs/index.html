<!doctype html>
<html lang="en">
  <head>
	<meta name="generator" content="Hugo 0.89.4" />
    <meta charset="utf-8">
    <meta content="IE=edge,chrome=1" http-equiv="X-UA-Compatible">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <title>
      Handover Document
    </title>
    








<link href='/DocRankingWebsite/styles/screen.min.dbadab296f1baed94fc5109ecef71e620384615bbb9c2c5fb0f0c9f10f900ab9.css' rel="stylesheet" media="screen" integrity="sha256-262rKW8brtlPxRCezvceYgOEYVu7nCxfsPDJ8Q&#43;QCrk=" />
<link href='/DocRankingWebsite/styles/print.min.e0db9e70d333a92515c28264c527870199194686a03c044b826e9827ec960b9d.css' rel="stylesheet" media="print" integrity="sha256-4NuecNMzqSUVwoJkxSeHAZkZRoagPARLgm6YJ&#43;yWC50=" />


    




  

<script src="/DocRankingWebsite/js/index.e4619e460f1bf032b63c9e8bf4c022ec118c03648d58c61e6a8402ae9e194017.js" integrity="sha256-5GGeRg8b8DK2PJ6L9MAi7BGMA2SNWMYeaoQCrp4ZQBc=" defer></script>

    


    <style>
      [x-cloak] {
      display: none !important;
      }
    </style>
  </head>
  <body class="index" x-data="{ navOpen: false }" x-cloak>
    <a href="#" id="nav-button" @click="navOpen = !navOpen" :class="{'open': navOpen }">
      <span>
        NAV
        <img src='/DocRankingWebsite/images/navbar.png'/>
      </span>
    </a>
    <div class="toc-wrapper" :class="{'open': navOpen }">
      
       <img src='/DocRankingWebsite/images/logo.png' class="logo" />
      
        <div class="search" x-data="searchController">
          <input x-model.debounce.100ms="query" type="search" class="search" id="input-search" placeholder='Search'>
          <ul class="search-results visible" x-show="results.length > 0" x-transition.duration.700ms >
            <template x-for="item in results">
              <li>
                <a x-text="item.title"></a>
              </li>
            </template>
          </ul>
        </div>
      
      
  
  
    
  
  <ul id="toc" class="toc-list-h1" x-data="tocController" x-init="load([{&#34;id&#34;:&#34;intro&#34;,&#34;level&#34;:1,&#34;sub&#34;:[],&#34;title&#34;:&#34;Intro&#34;},{&#34;id&#34;:&#34;barlow&#34;,&#34;level&#34;:1,&#34;sub&#34;:[{&#34;id&#34;:&#34;intro-1&#34;,&#34;level&#34;:2,&#34;title&#34;:&#34;Intro&#34;},{&#34;id&#34;:&#34;theory&#34;,&#34;level&#34;:2,&#34;title&#34;:&#34;Theory&#34;},{&#34;id&#34;:&#34;implementation&#34;,&#34;level&#34;:2,&#34;sub&#34;:[],&#34;title&#34;:&#34;Implementation&#34;}],&#34;title&#34;:&#34;Barlow&#34;},{&#34;id&#34;:&#34;bert&#34;,&#34;level&#34;:1,&#34;sub&#34;:[],&#34;title&#34;:&#34;BERT&#34;},{&#34;id&#34;:&#34;model&#34;,&#34;level&#34;:1,&#34;sub&#34;:[{&#34;id&#34;:&#34;bert-base&#34;,&#34;level&#34;:2,&#34;title&#34;:&#34;BERT Base&#34;},{&#34;id&#34;:&#34;barlow-head&#34;,&#34;level&#34;:2,&#34;title&#34;:&#34;Barlow Head&#34;},{&#34;id&#34;:&#34;ranking-head&#34;,&#34;level&#34;:2,&#34;title&#34;:&#34;Ranking Head&#34;},{&#34;id&#34;:&#34;pre-training-model&#34;,&#34;level&#34;:2,&#34;title&#34;:&#34;Pre-training Model&#34;},{&#34;id&#34;:&#34;training-model&#34;,&#34;level&#34;:2,&#34;sub&#34;:[],&#34;title&#34;:&#34;Training Model&#34;}],&#34;title&#34;:&#34;Model&#34;},{&#34;id&#34;:&#34;timeline&#34;,&#34;level&#34;:1,&#34;sub&#34;:[],&#34;title&#34;:&#34;Timeline&#34;},{&#34;id&#34;:&#34;discussion&#34;,&#34;level&#34;:1,&#34;sub&#34;:[{&#34;sub&#34;:[]}],&#34;title&#34;:&#34;Discussion&#34;}])" @scroll.window="onScroll()">
    
    <template x-for="row in rows">
      <li>
        <a x-text="row.title" @click="click(row)" :href="`#${row.id}`" class="toc-link" x-bind="rowClass(row)"></a>
        
          <ul x-show="row.open" x-bind="transitions()" class="da-toc-list-h2">
            <template x-for="row in row.sub">
              <li>
                <a x-text="row.title" @click="click(row)" :href="`#${row.id}`" class="toc-link" x-bind="rowClass(row)"></a>
                
                  <ul x-show="row.open" x-bind="transitions()" class="toc-list-h3">
                    <template x-for="row in row.sub">
                      <li>
                        <a  x-text="row.title" @click="click(row)" :href="`#${row.id}`"  class="toc-link" x-bind="rowClass(row)"></a>
                      </li>
                    </template>
                  </ul>
                
              </li>
            </template>
          </ul>
        
      </li>
    </template>
  </ul>

      
      
      

    </div>
    <div class="page-wrapper">
      <div class="dark-box"></div>
      <div class="content">
        
  
    
  
    
  
    
  
    
  
    
  
    
  
  
    





<h1 id="intro">Intro</h1><aside class="notice">
Introduction / Motivation
</aside>

  
    



  



<h1 id="barlow">Barlow</h1>



  
    
      
        
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  



<h2 id="intro-1">Intro</h2><p>Barlow Twins takes a batch of samples, applies noise to generate two distored versions, then passes both versions through two identical networks to get their corresponding embeddings. The Barlow loss is then computed on the embeddings, wherein the goal is to get the cross-correlation matrix between the embeddings as close as possible to the identity matrix. In this way, the embeddings of the two versions of the sample are encourraged to be similar, while redundancy between the components of the vectors is penalized.</p>
<p><em>&ldquo;Barlow Twins is competitive with state-of-the-art methods for self-supervised learning while being conceptually simpler, naturally avoiding trivial constant (i.e. collapsed) embeddings, and being robust to the training batch size.&quot;</em> From: <a href="https://arxiv.org/pdf/2103.03230.pdf">Barlow Twins: Self-Supervised Learning via Redundancy Reduction</a></p>
<p>In our project, we implement Barlow Twins to encourage similar embeddings between questions and conversations with the same <code>answer_id</code>. The assumption here is that questions and conversations that have the same answer must be related in some capacity, and moreover, that they must share an underlying concept. Hereby, the two &ldquo;versions&rdquo; can be likened to distortion of the same underlying concept.</p>




  



<h2 id="theory">Theory</h2><p>Whereas the original Barlow Paper takes a batch of samples and applies noise, in our implementation we already have the distorted matrices. For a given <code>answer_id</code>, this is simply a batch with associated questions (f), and a batch with associated conversations (g). In our dataloader, we ensure the batches are of equal size.</p>
<p><img src="https://nklingen.github.io/DocRankingWebsite//images/Barlow_1.png" alt="Barlow 1"></p>
<p>Now we compute the correlation between f and g.
<img src="https://nklingen.github.io/DocRankingWebsite//images/Barlow_2.png" alt="Barlow 2"></p>
<p>Having now computed the correlation matrix, we want to encourage it to resemble the identity matrix. Hereby, we have two terms. In the <code>invariance_term</code> we encourage the diagonals to be close to 1 and hereby for the model to be distortion agnostic, while in the <code>redundancy_reduction_term</code> we encourage all off-diagonals to be close to 0.</p>
<p><img src="https://nklingen.github.io/DocRankingWebsite//images/Barlow_3.png" alt="Barlow 3"></p>




  



<h2 id="implementation">Implementation</h2><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">
<span style="color:#66d9ef">for</span> batch <span style="color:#f92672">in</span> B:

    <span style="color:#75715e"># clear previously calculated gradients</span>
    optimizer<span style="color:#f92672">.</span>zero_grad()

    <span style="color:#75715e"># push the batch to device</span>
    batch <span style="color:#f92672">=</span> [r<span style="color:#f92672">.</span>to(device) <span style="color:#66d9ef">for</span> r <span style="color:#f92672">in</span> batch]
    input_encoding_ques, input_encoding_conv, _, _, attention_mask_conv, attention_mask_ques <span style="color:#f92672">=</span> batch

    barlow_sample_batch_size <span style="color:#f92672">=</span> input_encoding_ques<span style="color:#f92672">.</span>squeeze()<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]

    <span style="color:#75715e"># pass in tokens from question to get BERT output</span>
    f <span style="color:#f92672">=</span> model(
        input_encoding_ques<span style="color:#f92672">.</span>squeeze(), attention_mask_ques<span style="color:#f92672">.</span>squeeze()
    )  <span style="color:#75715e"># batch_size (10) x hidden size (768)</span>

    <span style="color:#75715e"># pass in tokens from conv to get BERT output</span>
    g <span style="color:#f92672">=</span> model(
        input_encoding_conv<span style="color:#f92672">.</span>squeeze(), attention_mask_conv<span style="color:#f92672">.</span>squeeze()
    )  <span style="color:#75715e"># batch_size (10) x hidden size (768)</span>

    <span style="color:#75715e"># normalize along the batch dimensions, thus we have the normalized features across all batches</span>
    f_norm <span style="color:#f92672">=</span> (f <span style="color:#f92672">-</span> f<span style="color:#f92672">.</span>mean(<span style="color:#ae81ff">0</span>)) <span style="color:#f92672">/</span> f<span style="color:#f92672">.</span>std(<span style="color:#ae81ff">0</span>)
    g_norm <span style="color:#f92672">=</span> (g <span style="color:#f92672">-</span> g<span style="color:#f92672">.</span>mean(<span style="color:#ae81ff">0</span>)) <span style="color:#f92672">/</span> g<span style="color:#f92672">.</span>std(<span style="color:#ae81ff">0</span>)

    <span style="color:#75715e"># cross-correlation matrix</span>
    c <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>matmul(f_norm<span style="color:#f92672">.</span>T, g_norm)<span style="color:#f92672">/</span> barlow_sample_batch_size

    invariance_term <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>diagonal(c)<span style="color:#f92672">.</span>add_(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>pow_(<span style="color:#ae81ff">2</span>)<span style="color:#f92672">.</span>sum()
    redundancy_reduction_term <span style="color:#f92672">=</span> off_diagonal(c)<span style="color:#f92672">.</span>pow_(<span style="color:#ae81ff">2</span>)<span style="color:#f92672">.</span>sum()
    loss <span style="color:#f92672">=</span> invariance_term <span style="color:#f92672">+</span> lambd <span style="color:#f92672">*</span> redundancy_reduction_term

</code></pre></div><p>Now to discuss our implementation. We create a specific dataloader for Barlow that passes through a batch of questions and a batch of conversations with the same <code>answer_id</code>, of equal size. Both batches are passed through the same model (a frozen BERT &amp; a trainable barlow head)</p>
<p>The the cross-correlation matrix c is computed as discussed previously, and finally normalized with the corresponding batch size.</p>
<p><code>c = (f - mean(f))*(g - mean(g))/(std(f) * std(g))/batch_size</code>,</p>
<p>The reason for normalising the cross-correlation matrix is because the batch_size is not constant between new <code>answer_ids</code>. Some <code>answer_ids</code> have only a few questions and conversations, where as others may have hundreds. Moreover, the two classes may be imbalanced for any given <code>answer_id</code>. To solve this, and to maximize the batch sizes, the data_loader first computes the maximal possible batch size for the <code>answer_id</code>.</p>
<p><code>batch_size = min(len(questions), len(conversations))</code></p>
<p>Then it takes all datapoints from the smaller set, and samples from the larger set until we have an equal number of data points from both sets.</p>
<p>Lastly, the <code>invariance_term</code> and <code>redundancy_term</code> are computed using two helper functions, <code>diagonal</code> and <code>off_diagonal</code> that return flattened versions of all elements in the diagonal or off-diagonal. The two terms are implemented exactly as described in <a href="#theory">Theory</a></p>

  
    



  



<h1 id="bert">BERT</h1><aside class="notice">
Nat : Brief section on how BERT was implemented
</aside>

  
    



  



<h1 id="model">Model</h1><blockquote>
<p>The two models are outlined below, where <a style="color:tomato">red</a> indicates sub-models with frozen parameters and where <a style="color:dodgerblue">blue</a> indicates sub-models with trainable parameters.</p>
</blockquote>
<blockquote>
<p><strong>Pre-training Model</strong></p>
<ol>
<li>
<p style="color:tomato"> BERT</p>
</li>
<li>
<p style="color:dodgerblue"> Barlow Head</p>
</li>
</ol>
</blockquote>
<blockquote>
<p><strong>Training Model</strong></p>
<ol>
<li>
<p style="color:tomato"> Pre-training Model</p>
<p style="color:tomato">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Bert<p>
<p style="color:tomato">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Barlow Head</p>
</li>
<li>
<p style="color:dodgerblue"> Ranking Head</p>
</li>
</ol>
</blockquote>
<p>To re-iterated, pre-training a Barlow head and then inheriting the head directly in the training loop led to poor results. While we saw that the shared head was converging very smoothly for the initial pre-training with Barlow Loss, and showed excellent results in pushing together the embeddings, it had very poor one-shot performance in Document Ranking, and did not converge more quickly.Consequently, we hypothesized that the model was not sufficiently complex to capture both the Barlow objective (push together question and conversation embeddings) and the Ranking objective (score highly on document ranking). That is, after training for the Barlow objective, the head was quickly overwritten when it was tasked with optimising for the Ranking objective. To counteract this (and thus get a true evaluation of applying Barlow Twins as a pretraining step), we decided to freeze the Barlow Twins head during training. Thus, the model learns to rank documents with a new head <em>without unlearning</em> the embeddings from the pretraining.</p>
<p>The following section will discuss how the pre-training and training models are constructed.</p>




  



<h2 id="bert-base">BERT Base</h2><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">BERT</span>(nn<span style="color:#f92672">.</span>Module):
    <span style="color:#66d9ef">def</span> __init__(self, config):

        super(BERT, self)<span style="color:#f92672">.</span>__init__()
        <span style="color:#75715e"># Reference: https://huggingface.co/Maltehb/danish-bert-botxo/blob/main/README.md</span>
        self<span style="color:#f92672">.</span>bert <span style="color:#f92672">=</span> AutoModel<span style="color:#f92672">.</span>from_pretrained(<span style="color:#e6db74">&#34;Maltehb/danish-bert-botxo&#34;</span>)

        <span style="color:#75715e"># freeze all the parameters in BERT. This prevents updating of model weights during fine-tuning</span>
        <span style="color:#66d9ef">for</span> param <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>bert<span style="color:#f92672">.</span>parameters():
            param<span style="color:#f92672">.</span>requires_grad <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>


    <span style="color:#75715e"># define the forward pass</span>
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, input_id, mask):

        <span style="color:#75715e"># Bert</span>
        BERT_output <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>bert(input_id, attention_mask<span style="color:#f92672">=</span>mask)

        <span style="color:#66d9ef">return</span> BERT_output
</code></pre></div><p>To delve further into the code, the BERT base simply passes the <code>input_id</code> and <code>attention_masks</code> from the Bert tokeniser through the Danish BERT model, downloaded from <a href="https://huggingface.co/Maltehb/danish-bert-botxo/blob/main/README.md">Hugging Face</a>.</p>
<p>We made an initial choice to only train the head (for time and complexity reasons), therefore in <code>init</code> we freeze all BERT paramters so they will never be updated.</p>




  



<h2 id="barlow-head">Barlow Head</h2><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">barlow_HEAD</span>(nn<span style="color:#f92672">.</span>Module):

    <span style="color:#66d9ef">def</span> __init__(self, config):
        super(barlow_HEAD, self)<span style="color:#f92672">.</span>__init__()
        <span style="color:#75715e"># dropout layer</span>
        self<span style="color:#f92672">.</span>dropout <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Dropout(config<span style="color:#f92672">.</span>dropout)
        <span style="color:#75715e"># relu activation function</span>
        self<span style="color:#f92672">.</span>relu <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>ReLU()
        <span style="color:#75715e"># dense layer 1</span>
        self<span style="color:#f92672">.</span>fc1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">768</span>, <span style="color:#ae81ff">768</span>)
        <span style="color:#75715e"># dense layer 2 (Output layer)</span>
        self<span style="color:#f92672">.</span>fc2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">768</span>, <span style="color:#ae81ff">768</span>)
        <span style="color:#75715e"># batch norm fc1</span>
        self<span style="color:#f92672">.</span>bn1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>BatchNorm1d(<span style="color:#ae81ff">768</span>)
        <span style="color:#75715e"># batch norm fc2</span>
        self<span style="color:#f92672">.</span>bn2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>BatchNorm1d(<span style="color:#ae81ff">768</span>)

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, BERT_output):

        <span style="color:#75715e"># capture cls</span>
        cls <span style="color:#f92672">=</span> BERT_output<span style="color:#f92672">.</span>last_hidden_state[:, <span style="color:#ae81ff">0</span>, :]
        <span style="color:#75715e"># FC 1 + batch norm</span>
        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>bn1(self<span style="color:#f92672">.</span>fc1(cls))
        <span style="color:#75715e"># relu activatiom</span>
        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>relu(x)
        <span style="color:#75715e"># dropout</span>
        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>dropout(x)
        <span style="color:#75715e"># FC 2 + batch norm</span>
        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>bn2(self<span style="color:#f92672">.</span>fc2(x))

        <span style="color:#66d9ef">return</span> x
</code></pre></div><p>The goal of this model is to push together question and conversation encodings.</p>
<p>We apply the following steps:</p>
<ol>
<li>capture the CLS token from the BERT output.</li>
<li>Pass through a fully connected layer with batch normalisation</li>
<li>Apply a ReLU activation function</li>
<li>Apply Dropout</li>
<li>Pass through a second fully connected layer with batch normalisation</li>
</ol>




  



<h2 id="ranking-head">Ranking Head</h2><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">ranking_HEAD</span>(nn<span style="color:#f92672">.</span>Module):

    <span style="color:#66d9ef">def</span> __init__(self, config):
        super(ranking_HEAD, self)<span style="color:#f92672">.</span>__init__()
        <span style="color:#75715e"># dropout layer</span>
        self<span style="color:#f92672">.</span>dropout <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Dropout(config<span style="color:#f92672">.</span>dropout)
        <span style="color:#75715e"># relu activation function</span>
        self<span style="color:#f92672">.</span>relu <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>ReLU()
        <span style="color:#75715e"># dense layer 1</span>
        self<span style="color:#f92672">.</span>fc1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">768</span>, <span style="color:#ae81ff">768</span>)
        <span style="color:#75715e"># dense layer 2 (Output layer)</span>
        self<span style="color:#f92672">.</span>fc2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">768</span>, <span style="color:#ae81ff">768</span>)
        <span style="color:#75715e"># batch norm fc1</span>
        self<span style="color:#f92672">.</span>bn1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>BatchNorm1d(<span style="color:#ae81ff">768</span>)
        <span style="color:#75715e"># batch norm fc2</span>
        self<span style="color:#f92672">.</span>bn2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>BatchNorm1d(<span style="color:#ae81ff">768</span>)

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):

        <span style="color:#75715e"># FC 1 + batch norm</span>
        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>bn1(self<span style="color:#f92672">.</span>fc1(x))
        <span style="color:#75715e"># relu activatiom</span>
        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>relu(x)
        <span style="color:#75715e"># dropout</span>
        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>dropout(x)
        <span style="color:#75715e"># FC 2 + batch norm</span>
        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>bn2(self<span style="color:#f92672">.</span>fc2(x))

        <span style="color:#66d9ef">return</span> x

</code></pre></div><p>The goal of this model is to pair question and conversation encodings with document encodings to optimise the Document Ranking score.</p>
<p>We apply the following steps:</p>
<ol>
<li>Pass through a fully connected layer with batch normalisation</li>
<li>Apply a ReLU activation function</li>
<li>Apply Dropout</li>
<li>Pass through a fully connected layer with batch normalisation</li>
</ol>




  



<h2 id="pre-training-model">Pre-training Model</h2><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">pretrainingModel</span>(nn<span style="color:#f92672">.</span>Module):

    <span style="color:#66d9ef">def</span> __init__(self, config):
        super(pretrainingModel, self)<span style="color:#f92672">.</span>__init__()
        self<span style="color:#f92672">.</span>bert <span style="color:#f92672">=</span> BERT(config)
        self<span style="color:#f92672">.</span>barlow_HEAD <span style="color:#f92672">=</span> barlow_HEAD(config)

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, input_id, mask):
        BERT_output <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>bert(input_id, mask)
        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>barlow_HEAD(BERT_output)

        <span style="color:#66d9ef">return</span> x
</code></pre></div><p>The pre-training model simply passes the <code>input_id</code> and <code>mask</code> from the tokeniser through the bert model and the barlow head. The BERT parameters are frozen in the <a href="#bert-base">Bert Base</a> <code>init</code>, so only the <a href="#barlow-head">Barlow Head</a> parameters train.</p>




  



<h2 id="training-model">Training Model</h2><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">trainingModel</span>(nn<span style="color:#f92672">.</span>Module):

    <span style="color:#66d9ef">def</span> __init__(self, pretrainingModel: pretrainingModel, config):
        super(trainingModel, self)<span style="color:#f92672">.</span>__init__()

        <span style="color:#75715e"># inherit the BERT and BarlowHEAD from pretraining model</span>
        self<span style="color:#f92672">.</span>pretrainingModel <span style="color:#f92672">=</span> pretrainingModel
        <span style="color:#75715e"># freeze all the parameters in barlow_HEAD. </span>
        <span style="color:#75715e"># This embedding head shouldn&#39;t learn a new task. </span>
        <span style="color:#75715e"># Ranking head needs to adapt to embedding provided.</span>
        <span style="color:#66d9ef">for</span> param <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>pretrainingModel<span style="color:#f92672">.</span>parameters():
            param<span style="color:#f92672">.</span>requires_grad <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>
            
        self<span style="color:#f92672">.</span>ranking_HEAD <span style="color:#f92672">=</span> ranking_HEAD(config)
    
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">set_pretrain_model_to_eval</span>(self):
        self<span style="color:#f92672">.</span>pretrainingModel<span style="color:#f92672">.</span>eval()
    
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, input_id, mask):
        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>pretrainingModel(input_id, mask)
        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>ranking_HEAD(x)

        <span style="color:#66d9ef">return</span> x
</code></pre></div><p>In the Training-Model, the <a href="pre-training-model">pre-training model</a> (including both <a href="#bert-base">Bert Base</a> and <a href="#barlow-head">Barlow Head</a>) is passed as a parameter. All model parameters are frozen, such that the training model will not update parameters from the pre-training model. During a forward pass, input is passed through the frozen pre-training model and then through an addition <a href="#ranking-head">Ranking Head</a> which is trained with the objective of optimising document ranking scores.</p>
<p>Moreover, the training-model includes a method <code>set_pretrain_model_to_eval</code> that is called from the main script. When the Training Model is set to train mode, we want to ensure that only the ranking head is actually in train mode, while all frozen pre-training sub-models remain in eval mode. Thus, <code>set_pretrain_model_to_eval</code> sets the <a href="pre-training-model">pre-training model</a> back to eval.</p>

  
    



  



<h1 id="timeline">Timeline</h1><aside class="notice">
Mikkel's portion on what we tried
</aside>
  
    



  



<h1 id="discussion">Discussion</h1><aside class="notice">
Nat \ Mikkel
</aside>
<p>We can see that Barlow Head accomplishes what it needs to do. (it works) but still that it breaks the downstream BERT task. We tried both frozen and unfrozen heads, and in both cases BERT is much worse off with the pretraining than with random initialized weights. We made sure it can&rsquo;t be a scaling issue as everything is normalized.</p>
<p>Take-away, BERT makes its own representations in the encoding space that work well for document ranking, and tweaking these seems to hurt the model. Maybe moving questions and conversations together is an idea that makes sense from a human perspective, but interferes with the models deeper understanding of the datatypes. We don&rsquo;t deeply understand how the model represents the data.</p>

  

      </div>
      <div class="dark-box">
        
          <div class="lang-selector" x-data="langController" x-init="initLangs([{&#34;key&#34;:&#34;python&#34;,&#34;name&#34;:&#34;Python&#34;}])">
            <template x-for="(tab, index) in tabs">
              <a x-text="tab.name" :class="{ 'active': tab.active }" @click="changeLanguage(index)"></a>
            </template>
          </div>
        
      </div>
    </div>
    
  </body>
</html>
