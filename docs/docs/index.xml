<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Docs on Handover Document</title>
    <link>https://nklingen.github.io/DocRankingWebsite/docs/</link>
    <description>Recent content in Docs on Handover Document</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="https://nklingen.github.io/DocRankingWebsite/docs/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Intro</title>
      <link>https://nklingen.github.io/DocRankingWebsite/docs/intro/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://nklingen.github.io/DocRankingWebsite/docs/intro/</guid>
      <description>Intro AboutThis website is a Handover Document by Mikkel Odgaard and Natasha Klingenbrunn for a collaboration project between Denmark&amp;rsquo;s Technical University and Raffle.ai. The final code can be found on Github.
The website is broken down as follows:
 A brief conceptual background is given for Barlow Twins and BERT. Next, the Timeline section will discuss the evolution of our project and discuss the rationale behind given design choices. The code break-down for our model and barlow loss can be found in their respective sections.</description>
    </item>
    
    <item>
      <title>Metrics</title>
      <link>https://nklingen.github.io/DocRankingWebsite/docs/metrics/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://nklingen.github.io/DocRankingWebsite/docs/metrics/</guid>
      <description>Metrics To compute top k accuracy
 def k_accuracy(scores, answer_ids, target_answer_ids, k): assert k &amp;gt;= 1 if k == 1: prediction_indices = torch.argmax(scores, dim=1) prediction_answer_ids = answer_ids[prediction_indices] return (prediction_answer_ids == target_answer_ids) elif k &amp;gt; 1: prediction_indices = torch.topk(scores, k).indices prediction_answer_ids = answer_ids[prediction_indices] return (prediction_answer_ids == target_answer_ids.unsqueeze(1)).any(1)  To compute top k average precision score (from Raffle&amp;rsquo;s codebase)
 def batch_map(relevance, k): top_k = relevance[:, :k] batch_size = top_k.size(0) pos = torch.</description>
    </item>
    
    <item>
      <title>Timeline</title>
      <link>https://nklingen.github.io/DocRankingWebsite/docs/timeline/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://nklingen.github.io/DocRankingWebsite/docs/timeline/</guid>
      <description>TimelineThis section will be a general summary of our work and the evolution of the project. The project went through 3 overall phases in regards to the construction of the model.
 Exploratory Data Analysis (EDA) Document Ranking Model Barlow Twins  Exploratory Data AnalysisThe input data contains 4 main pieces of information. The title, the content, the type (questions or conversations) and answer id (the &amp;ldquo;true&amp;rdquo; label). Each piece of information should be analysed due to its importance.</description>
    </item>
    
    <item>
      <title>Model</title>
      <link>https://nklingen.github.io/DocRankingWebsite/docs/model/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://nklingen.github.io/DocRankingWebsite/docs/model/</guid>
      <description>Model The two models are outlined below, where red indicates sub-models with frozen parameters and where blue indicates sub-models with trainable parameters.
  Pre-training Model
  BERT
  Barlow Head
    Training Model
  Pre-training Model
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; Bert &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; Barlow Head
  Ranking Head
   The following section will discuss how the pre-training (with frozen head) and training models are constructed.</description>
    </item>
    
    <item>
      <title>Discussion</title>
      <link>https://nklingen.github.io/DocRankingWebsite/docs/discussion/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://nklingen.github.io/DocRankingWebsite/docs/discussion/</guid>
      <description>DiscussionThe main takeaway was that we could see that both the Document Ranking BERT and the Barlow models worked seperately. However, as soon as we coupled them, the model performed significantly worse than the original baseline. In fact, the starting accuracy was nearly 0, giving the same performance as randomly guessing.
We tried with both frozen and unfrozen parameters being passed from the pretraining model to the training model, but in neither case could we see any improvement.</description>
    </item>
    
  </channel>
</rss>
