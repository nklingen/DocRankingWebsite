<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta content="IE=edge,chrome=1" http-equiv="X-UA-Compatible">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <title>
      Docs
    </title>
    








<link href='/DocRankingWebsite/styles/screen.min.dbadab296f1baed94fc5109ecef71e620384615bbb9c2c5fb0f0c9f10f900ab9.css' rel="stylesheet" media="screen" integrity="sha256-262rKW8brtlPxRCezvceYgOEYVu7nCxfsPDJ8Q&#43;QCrk=" />
<link href='/DocRankingWebsite/styles/print.min.e0db9e70d333a92515c28264c527870199194686a03c044b826e9827ec960b9d.css' rel="stylesheet" media="print" integrity="sha256-4NuecNMzqSUVwoJkxSeHAZkZRoagPARLgm6YJ&#43;yWC50=" />


    




  

<script src="/DocRankingWebsite/js/index.e4619e460f1bf032b63c9e8bf4c022ec118c03648d58c61e6a8402ae9e194017.js" integrity="sha256-5GGeRg8b8DK2PJ6L9MAi7BGMA2SNWMYeaoQCrp4ZQBc=" defer></script>

    


    <style>
      [x-cloak] {
      display: none !important;
      }
    </style>
  </head>
  <body class="index" x-data="{ navOpen: false }" x-cloak>
    <a href="#" id="nav-button" @click="navOpen = !navOpen" :class="{'open': navOpen }">
      <span>
        NAV
        <img src='/DocRankingWebsite/images/navbar.png'/>
      </span>
    </a>
    <div class="toc-wrapper" :class="{'open': navOpen }">
      
       <img src='/DocRankingWebsite/images/logo.png' class="logo" />
      
        <div class="search" x-data="searchController">
          <input x-model.debounce.100ms="query" type="search" class="search" id="input-search" placeholder='Search'>
          <ul class="search-results visible" x-show="results.length > 0" x-transition.duration.700ms >
            <template x-for="item in results">
              <li>
                <a x-text="item.title"></a>
              </li>
            </template>
          </ul>
        </div>
      
      
  
  
    
  
  <ul id="toc" class="toc-list-h1" x-data="tocController" x-init="load([{&#34;id&#34;:&#34;intro&#34;,&#34;level&#34;:1,&#34;sub&#34;:[{&#34;id&#34;:&#34;about&#34;,&#34;level&#34;:2,&#34;title&#34;:&#34;About&#34;},{&#34;id&#34;:&#34;motivation&#34;,&#34;level&#34;:2,&#34;sub&#34;:[],&#34;title&#34;:&#34;Motivation&#34;}],&#34;title&#34;:&#34;Intro&#34;},{&#34;id&#34;:&#34;barlow&#34;,&#34;level&#34;:1,&#34;sub&#34;:[{&#34;id&#34;:&#34;intro-1&#34;,&#34;level&#34;:2,&#34;title&#34;:&#34;Intro&#34;},{&#34;id&#34;:&#34;theory&#34;,&#34;level&#34;:2,&#34;title&#34;:&#34;Theory&#34;},{&#34;id&#34;:&#34;implementation&#34;,&#34;level&#34;:2,&#34;sub&#34;:[],&#34;title&#34;:&#34;Implementation&#34;}],&#34;title&#34;:&#34;Barlow&#34;},{&#34;id&#34;:&#34;bert&#34;,&#34;level&#34;:1,&#34;sub&#34;:[],&#34;title&#34;:&#34;BERT&#34;},{&#34;id&#34;:&#34;model&#34;,&#34;level&#34;:1,&#34;sub&#34;:[{&#34;id&#34;:&#34;bert-base&#34;,&#34;level&#34;:2,&#34;title&#34;:&#34;BERT Base&#34;},{&#34;id&#34;:&#34;barlow-head&#34;,&#34;level&#34;:2,&#34;title&#34;:&#34;Barlow Head&#34;},{&#34;id&#34;:&#34;ranking-head&#34;,&#34;level&#34;:2,&#34;title&#34;:&#34;Ranking Head&#34;},{&#34;id&#34;:&#34;pre-training-model&#34;,&#34;level&#34;:2,&#34;title&#34;:&#34;Pre-training Model&#34;},{&#34;id&#34;:&#34;training-model&#34;,&#34;level&#34;:2,&#34;sub&#34;:[],&#34;title&#34;:&#34;Training Model&#34;}],&#34;title&#34;:&#34;Model&#34;},{&#34;id&#34;:&#34;timeline&#34;,&#34;level&#34;:1,&#34;sub&#34;:[{&#34;id&#34;:&#34;1-fundamental-construction&#34;,&#34;level&#34;:2,&#34;title&#34;:&#34;1. Fundamental construction&#34;},{&#34;id&#34;:&#34;2--exploratory-data-analysis&#34;,&#34;level&#34;:2,&#34;sub&#34;:[{&#34;id&#34;:&#34;21-answer-ids&#34;,&#34;level&#34;:3,&#34;title&#34;:&#34;2.1 Answer ids&#34;},{&#34;id&#34;:&#34;22-dataset-imbalance&#34;,&#34;level&#34;:3,&#34;title&#34;:&#34;2.2 Dataset imbalance&#34;},{&#34;id&#34;:&#34;23-pca&#34;,&#34;level&#34;:3,&#34;title&#34;:&#34;2.3 PCA&#34;}],&#34;title&#34;:&#34;2.  Exploratory Data Analysis&#34;},{&#34;id&#34;:&#34;3-generic-improvements&#34;,&#34;level&#34;:2,&#34;sub&#34;:[{&#34;id&#34;:&#34;31-preprocessing&#34;,&#34;level&#34;:3,&#34;title&#34;:&#34;3.1 Preprocessing&#34;},{&#34;id&#34;:&#34;32-dual-encoder&#34;,&#34;level&#34;:3,&#34;title&#34;:&#34;3.2 Dual encoder&#34;},{&#34;id&#34;:&#34;33-gradient-clipping-l2-regularization--batch-normalization&#34;,&#34;level&#34;:3,&#34;title&#34;:&#34;3.3 Gradient clipping, L2 regularization \u0026amp; batch normalization&#34;}],&#34;title&#34;:&#34;3. Generic improvements&#34;},{&#34;id&#34;:&#34;4-barlow-twins&#34;,&#34;level&#34;:2,&#34;sub&#34;:[],&#34;title&#34;:&#34;4. Barlow Twins&#34;}],&#34;title&#34;:&#34;Timeline&#34;},{&#34;id&#34;:&#34;discussion&#34;,&#34;level&#34;:1,&#34;sub&#34;:[{&#34;sub&#34;:[]}],&#34;title&#34;:&#34;Discussion&#34;}])" @scroll.window="onScroll()">
    
    <template x-for="row in rows">
      <li>
        <a x-text="row.title" @click="click(row)" :href="`#${row.id}`" class="toc-link" x-bind="rowClass(row)"></a>
        
          <ul x-show="row.open" x-bind="transitions()" class="da-toc-list-h2">
            <template x-for="row in row.sub">
              <li>
                <a x-text="row.title" @click="click(row)" :href="`#${row.id}`" class="toc-link" x-bind="rowClass(row)"></a>
                
                  <ul x-show="row.open" x-bind="transitions()" class="toc-list-h3">
                    <template x-for="row in row.sub">
                      <li>
                        <a  x-text="row.title" @click="click(row)" :href="`#${row.id}`"  class="toc-link" x-bind="rowClass(row)"></a>
                      </li>
                    </template>
                  </ul>
                
              </li>
            </template>
          </ul>
        
      </li>
    </template>
  </ul>

      
      
      

    </div>
    <div class="page-wrapper">
      <div class="dark-box"></div>
      <div class="content">
        
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
  
    





<h1 id="intro">Intro</h1>



  



<h2 id="about">About</h2><p>This website is a Handover Document by <a href="https://github.com/mikkelfo">Mikkel Odgaard</a> and <a href="https://github.com/nklingen">Natasha Klingenbrunn</a> for a collaboration project between Denmark&rsquo;s Technical University and <a href="https://www.raffle.ai">Raffle.ai</a>. The <a href="https://github.com/nklingen/BERT_Question_Answering">final code</a> can be found on Github.</p>
<p>The website is broken down as follows:</p>
<ol>
<li>A brief conceptual background is given for Barlow Twins and BERT.</li>
<li>Next, the Timeline section will discuss the evolution of our project and discuss the rationale behind given design choices.</li>
<li>The code break-down for our model and barlow loss can be found in their respective sections.</li>
<li>Finally, the discussion section will outline why believe our model was not successful, and touch upon our final thoughts.</li>
</ol>




  



<h2 id="motivation">Motivation</h2><p>Raffle.ai&rsquo;s BERT Document Ranking Model sees better performance on questions than on conversations. The motivation behind this project is to leverage SSL to yield a more constant performance across input types. To accomplish this, we will introduce a pre-training step implementing Barlow Twins to push encodings together for similar questions and conversations. Having a successful implementation of the pre-training model will allow us to analyse the downstream effects in the BERT Ranking Model.</p>
<p>For example, one hypothesis prior to beginning this project was that the BERT Ranking Model might project question and conversations into two seperate planes. Thus, while it may be very successful in one input-type, this does not translate to high performance for the other. We hope to learn whether having more similar embeddings for questions and conversations has a positive effect on the model, or whether the model&rsquo;s own learned representations are better. While we didn&rsquo;t find it likely to have a better initial performance (if projecting all inputs from the same answer_id together had a lower loss in the downstream task, it would have done it anyways), but hoped that we could see other indicators of pre-training benefitting the model, such as a quicker rate of converging, for example.</p>

  
    



  



<h1 id="barlow">Barlow</h1>



  
    
      
        
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  



<h2 id="intro-1">Intro</h2><p>Barlow Twins takes a batch of samples, applies noise to generate two distored versions, then passes both versions through two identical networks to get their corresponding embeddings. The Barlow loss is then computed on the embeddings, wherein the goal is to get the cross-correlation matrix between the embeddings as close as possible to the identity matrix. In this way, the embeddings of the two versions of the sample are encourraged to be similar, while redundancy between the components of the vectors is penalized.</p>
<p><em>&ldquo;Barlow Twins is competitive with state-of-the-art methods for self-supervised learning while being conceptually simpler, naturally avoiding trivial constant (i.e. collapsed) embeddings, and being robust to the training batch size.&quot;</em> From: <a href="https://arxiv.org/pdf/2103.03230.pdf">Barlow Twins: Self-Supervised Learning via Redundancy Reduction</a></p>
<p>In our project, we implement Barlow Twins to encourage similar embeddings between questions and conversations with the same <code>answer_id</code>. The assumption here is that questions and conversations that have the same answer must be related in some capacity, and moreover, that they must share an underlying concept. Hereby, the two &ldquo;versions&rdquo; can be likened to distortion of the same underlying concept.</p>




  



<h2 id="theory">Theory</h2><p>Whereas the original Barlow Paper takes a batch of samples and applies noise, in our implementation we already have the distorted matrices. For a given <code>answer_id</code>, this is simply a batch with associated questions (f), and a batch with associated conversations (g). In our dataloader, we ensure the batches are of equal size.</p>
<p><img src="https://nklingen.github.io/DocRankingWebsite//images/Barlow_1.png" alt="Barlow 1"></p>
<p>Now we compute the correlation between f and g.
<img src="https://nklingen.github.io/DocRankingWebsite//images/Barlow_eq.png" alt="Barlow EQ"></p>
<p>This yields the following correlation matrix:
<img src="https://nklingen.github.io/DocRankingWebsite//images/Barlow_matrix.png" alt="Barlow Matrix"></p>
<p>Having now computed the correlation matrix, we want to encourage it to resemble the identity matrix. Hereby, we have two terms. In the <code>invariance_term</code> we encourage the diagonals (marked in grey) to be close to 1 and hereby for the model to be distortion agnostic, while in the <code>redundancy_reduction_term</code> we encourage all off-diagonals to be close to 0.</p>
<p><img src="https://nklingen.github.io/DocRankingWebsite//images/Barlow_3.png" alt="Barlow 3"></p>




  



<h2 id="implementation">Implementation</h2><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">
<span style="color:#66d9ef">for</span> batch <span style="color:#f92672">in</span> B:

    <span style="color:#75715e"># clear previously calculated gradients</span>
    optimizer<span style="color:#f92672">.</span>zero_grad()

    <span style="color:#75715e"># push the batch to device</span>
    batch <span style="color:#f92672">=</span> [r<span style="color:#f92672">.</span>to(device) <span style="color:#66d9ef">for</span> r <span style="color:#f92672">in</span> batch]
    input_encoding_ques, input_encoding_conv, _, _, attention_mask_conv, attention_mask_ques <span style="color:#f92672">=</span> batch

    barlow_sample_batch_size <span style="color:#f92672">=</span> input_encoding_ques<span style="color:#f92672">.</span>squeeze()<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]

    <span style="color:#75715e"># pass in tokens from question to get BERT output</span>
    f <span style="color:#f92672">=</span> model(
        input_encoding_ques<span style="color:#f92672">.</span>squeeze(), attention_mask_ques<span style="color:#f92672">.</span>squeeze()
    )  <span style="color:#75715e"># batch_size (10) x hidden size (768)</span>

    <span style="color:#75715e"># pass in tokens from conv to get BERT output</span>
    g <span style="color:#f92672">=</span> model(
        input_encoding_conv<span style="color:#f92672">.</span>squeeze(), attention_mask_conv<span style="color:#f92672">.</span>squeeze()
    )  <span style="color:#75715e"># batch_size (10) x hidden size (768)</span>

    <span style="color:#75715e"># normalize along the batch dimensions, thus we have the normalized features across all batches</span>
    f_norm <span style="color:#f92672">=</span> (f <span style="color:#f92672">-</span> f<span style="color:#f92672">.</span>mean(<span style="color:#ae81ff">0</span>)) <span style="color:#f92672">/</span> f<span style="color:#f92672">.</span>std(<span style="color:#ae81ff">0</span>)
    g_norm <span style="color:#f92672">=</span> (g <span style="color:#f92672">-</span> g<span style="color:#f92672">.</span>mean(<span style="color:#ae81ff">0</span>)) <span style="color:#f92672">/</span> g<span style="color:#f92672">.</span>std(<span style="color:#ae81ff">0</span>)

    <span style="color:#75715e"># cross-correlation matrix</span>
    c <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>matmul(f_norm<span style="color:#f92672">.</span>T, g_norm)<span style="color:#f92672">/</span> barlow_sample_batch_size

    invariance_term <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>diagonal(c)<span style="color:#f92672">.</span>add_(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>pow_(<span style="color:#ae81ff">2</span>)<span style="color:#f92672">.</span>sum()
    redundancy_reduction_term <span style="color:#f92672">=</span> off_diagonal(c)<span style="color:#f92672">.</span>pow_(<span style="color:#ae81ff">2</span>)<span style="color:#f92672">.</span>sum()
    loss <span style="color:#f92672">=</span> invariance_term <span style="color:#f92672">+</span> lambd <span style="color:#f92672">*</span> redundancy_reduction_term

</code></pre></div><p>Now to discuss our implementation. We create a specific dataloader for Barlow that passes through a batch of questions and a batch of conversations with the same <code>answer_id</code>, of equal size. Both batches are passed through the same model (a frozen BERT &amp; a trainable barlow head)</p>
<p>The the cross-correlation matrix c is computed as discussed previously, and finally normalized with the corresponding batch size.</p>
<p><code>c = (f - mean(f))*(g - mean(g))/(std(f) * std(g))/batch_size</code>,</p>
<p>The reason for normalising the cross-correlation matrix is because the batch_size is not constant between new <code>answer_ids</code>. Some <code>answer_ids</code> have only a few questions and conversations, where as others may have hundreds. Moreover, the two classes may be imbalanced for any given <code>answer_id</code>. To solve this, and to maximize the batch sizes, the data_loader first computes the maximal possible batch size for the <code>answer_id</code>.</p>
<p><code>batch_size = min(len(questions), len(conversations))</code></p>
<p>Then it takes all datapoints from the smaller set, and samples from the larger set until we have an equal number of data points from both sets.</p>
<p>Lastly, the <code>invariance_term</code> and <code>redundancy_term</code> are computed using two helper functions, <code>diagonal</code> and <code>off_diagonal</code> that return flattened versions of all elements in the diagonal or off-diagonal. The two terms are implemented exactly as described in <a href="#theory">Theory</a></p>

  
    



  



<h1 id="bert">BERT</h1><aside class="notice">
Nat : Brief section on how BERT was implemented
</aside>

  
    



  



<h1 id="model">Model</h1><blockquote>
<p>The two models are outlined below, where <a style="color:tomato">red</a> indicates sub-models with frozen parameters and where <a style="color:dodgerblue">blue</a> indicates sub-models with trainable parameters.</p>
</blockquote>
<blockquote>
<p><strong>Pre-training Model</strong></p>
<ol>
<li>
<p style="color:tomato"> BERT</p>
</li>
<li>
<p style="color:dodgerblue"> Barlow Head</p>
</li>
</ol>
</blockquote>
<blockquote>
<p><strong>Training Model</strong></p>
<ol>
<li>
<p style="color:tomato"> Pre-training Model</p>
<p style="color:tomato">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Bert<p>
<p style="color:tomato">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Barlow Head</p>
</li>
<li>
<p style="color:dodgerblue"> Ranking Head</p>
</li>
</ol>
</blockquote>
<p>To re-iterated, pre-training a Barlow head and then inheriting the head directly in the training loop led to poor results. While we saw that the shared head was converging very smoothly for the initial pre-training with Barlow Loss, and showed excellent results in pushing together the embeddings, it had very poor one-shot performance in Document Ranking, and did not converge more quickly.Consequently, we hypothesized that the model was not sufficiently complex to capture both the Barlow objective (push together question and conversation embeddings) and the Ranking objective (score highly on document ranking). That is, after training for the Barlow objective, the head was quickly overwritten when it was tasked with optimising for the Ranking objective. To counteract this (and thus get a true evaluation of applying Barlow Twins as a pretraining step), we decided to freeze the Barlow Twins head during training. Thus, the model learns to rank documents with a new head <em>without unlearning</em> the embeddings from the pretraining.</p>
<p>The following section will discuss how the pre-training and training models are constructed.</p>




  



<h2 id="bert-base">BERT Base</h2><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">BERT</span>(nn<span style="color:#f92672">.</span>Module):
    <span style="color:#66d9ef">def</span> __init__(self, config):

        super(BERT, self)<span style="color:#f92672">.</span>__init__()
        <span style="color:#75715e"># Reference: https://huggingface.co/Maltehb/danish-bert-botxo/blob/main/README.md</span>
        self<span style="color:#f92672">.</span>bert <span style="color:#f92672">=</span> AutoModel<span style="color:#f92672">.</span>from_pretrained(<span style="color:#e6db74">&#34;Maltehb/danish-bert-botxo&#34;</span>)

        <span style="color:#75715e"># freeze all the parameters in BERT. This prevents updating of model weights during fine-tuning</span>
        <span style="color:#66d9ef">for</span> param <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>bert<span style="color:#f92672">.</span>parameters():
            param<span style="color:#f92672">.</span>requires_grad <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>


    <span style="color:#75715e"># define the forward pass</span>
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, input_id, mask):

        <span style="color:#75715e"># Bert</span>
        BERT_output <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>bert(input_id, attention_mask<span style="color:#f92672">=</span>mask)

        <span style="color:#66d9ef">return</span> BERT_output
</code></pre></div><p>To delve further into the code, the BERT base simply passes the <code>input_id</code> and <code>attention_masks</code> from the Bert tokeniser through the Danish BERT model, downloaded from <a href="https://huggingface.co/Maltehb/danish-bert-botxo/blob/main/README.md">Hugging Face</a>.</p>
<p>We made an initial choice to only train the head (for time and complexity reasons), therefore in <code>init</code> we freeze all BERT paramters so they will never be updated.</p>




  



<h2 id="barlow-head">Barlow Head</h2><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">barlow_HEAD</span>(nn<span style="color:#f92672">.</span>Module):

    <span style="color:#66d9ef">def</span> __init__(self, config):
        super(barlow_HEAD, self)<span style="color:#f92672">.</span>__init__()
        <span style="color:#75715e"># dropout layer</span>
        self<span style="color:#f92672">.</span>dropout <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Dropout(config<span style="color:#f92672">.</span>dropout)
        <span style="color:#75715e"># relu activation function</span>
        self<span style="color:#f92672">.</span>relu <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>ReLU()
        <span style="color:#75715e"># dense layer 1</span>
        self<span style="color:#f92672">.</span>fc1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">768</span>, <span style="color:#ae81ff">768</span>)
        <span style="color:#75715e"># dense layer 2 (Output layer)</span>
        self<span style="color:#f92672">.</span>fc2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">768</span>, <span style="color:#ae81ff">768</span>)
        <span style="color:#75715e"># batch norm fc1</span>
        self<span style="color:#f92672">.</span>bn1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>BatchNorm1d(<span style="color:#ae81ff">768</span>)
        <span style="color:#75715e"># batch norm fc2</span>
        self<span style="color:#f92672">.</span>bn2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>BatchNorm1d(<span style="color:#ae81ff">768</span>)

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, BERT_output):

        <span style="color:#75715e"># capture cls</span>
        cls <span style="color:#f92672">=</span> BERT_output<span style="color:#f92672">.</span>last_hidden_state[:, <span style="color:#ae81ff">0</span>, :]
        <span style="color:#75715e"># FC 1 + batch norm</span>
        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>bn1(self<span style="color:#f92672">.</span>fc1(cls))
        <span style="color:#75715e"># relu activatiom</span>
        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>relu(x)
        <span style="color:#75715e"># dropout</span>
        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>dropout(x)
        <span style="color:#75715e"># FC 2 + batch norm</span>
        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>bn2(self<span style="color:#f92672">.</span>fc2(x))

        <span style="color:#66d9ef">return</span> x
</code></pre></div><p>The goal of this model is to push together question and conversation encodings.</p>
<p>We apply the following steps:</p>
<ol>
<li>capture the CLS token from the BERT output.</li>
<li>Pass through a fully connected layer with batch normalisation</li>
<li>Apply a ReLU activation function</li>
<li>Apply Dropout</li>
<li>Pass through a second fully connected layer with batch normalisation</li>
</ol>




  



<h2 id="ranking-head">Ranking Head</h2><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">ranking_HEAD</span>(nn<span style="color:#f92672">.</span>Module):

    <span style="color:#66d9ef">def</span> __init__(self, config):
        super(ranking_HEAD, self)<span style="color:#f92672">.</span>__init__()
        <span style="color:#75715e"># dropout layer</span>
        self<span style="color:#f92672">.</span>dropout <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Dropout(config<span style="color:#f92672">.</span>dropout)
        <span style="color:#75715e"># relu activation function</span>
        self<span style="color:#f92672">.</span>relu <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>ReLU()
        <span style="color:#75715e"># dense layer 1</span>
        self<span style="color:#f92672">.</span>fc1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">768</span>, <span style="color:#ae81ff">768</span>)
        <span style="color:#75715e"># dense layer 2 (Output layer)</span>
        self<span style="color:#f92672">.</span>fc2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">768</span>, <span style="color:#ae81ff">768</span>)
        <span style="color:#75715e"># batch norm fc1</span>
        self<span style="color:#f92672">.</span>bn1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>BatchNorm1d(<span style="color:#ae81ff">768</span>)
        <span style="color:#75715e"># batch norm fc2</span>
        self<span style="color:#f92672">.</span>bn2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>BatchNorm1d(<span style="color:#ae81ff">768</span>)

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):

        <span style="color:#75715e"># FC 1 + batch norm</span>
        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>bn1(self<span style="color:#f92672">.</span>fc1(x))
        <span style="color:#75715e"># relu activatiom</span>
        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>relu(x)
        <span style="color:#75715e"># dropout</span>
        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>dropout(x)
        <span style="color:#75715e"># FC 2 + batch norm</span>
        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>bn2(self<span style="color:#f92672">.</span>fc2(x))

        <span style="color:#66d9ef">return</span> x

</code></pre></div><p>The goal of this model is to pair question and conversation encodings with document encodings to optimise the Document Ranking score.</p>
<p>We apply the following steps:</p>
<ol>
<li>Pass through a fully connected layer with batch normalisation</li>
<li>Apply a ReLU activation function</li>
<li>Apply Dropout</li>
<li>Pass through a fully connected layer with batch normalisation</li>
</ol>




  



<h2 id="pre-training-model">Pre-training Model</h2><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">pretrainingModel</span>(nn<span style="color:#f92672">.</span>Module):

    <span style="color:#66d9ef">def</span> __init__(self, config):
        super(pretrainingModel, self)<span style="color:#f92672">.</span>__init__()
        self<span style="color:#f92672">.</span>bert <span style="color:#f92672">=</span> BERT(config)
        self<span style="color:#f92672">.</span>barlow_HEAD <span style="color:#f92672">=</span> barlow_HEAD(config)

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, input_id, mask):
        BERT_output <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>bert(input_id, mask)
        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>barlow_HEAD(BERT_output)

        <span style="color:#66d9ef">return</span> x
</code></pre></div><p>The pre-training model simply passes the <code>input_id</code> and <code>mask</code> from the tokeniser through the bert model and the barlow head. The BERT parameters are frozen in the <a href="#bert-base">Bert Base</a> <code>init</code>, so only the <a href="#barlow-head">Barlow Head</a> parameters train.</p>




  



<h2 id="training-model">Training Model</h2><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">trainingModel</span>(nn<span style="color:#f92672">.</span>Module):

    <span style="color:#66d9ef">def</span> __init__(self, pretrainingModel: pretrainingModel, config):
        super(trainingModel, self)<span style="color:#f92672">.</span>__init__()

        <span style="color:#75715e"># inherit the BERT and BarlowHEAD from pretraining model</span>
        self<span style="color:#f92672">.</span>pretrainingModel <span style="color:#f92672">=</span> pretrainingModel
        <span style="color:#75715e"># freeze all the parameters in barlow_HEAD. </span>
        <span style="color:#75715e"># This embedding head shouldn&#39;t learn a new task. </span>
        <span style="color:#75715e"># Ranking head needs to adapt to embedding provided.</span>
        <span style="color:#66d9ef">for</span> param <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>pretrainingModel<span style="color:#f92672">.</span>parameters():
            param<span style="color:#f92672">.</span>requires_grad <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>
            
        self<span style="color:#f92672">.</span>ranking_HEAD <span style="color:#f92672">=</span> ranking_HEAD(config)
    
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">set_pretrain_model_to_eval</span>(self):
        self<span style="color:#f92672">.</span>pretrainingModel<span style="color:#f92672">.</span>eval()
    
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, input_id, mask):
        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>pretrainingModel(input_id, mask)
        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>ranking_HEAD(x)

        <span style="color:#66d9ef">return</span> x
</code></pre></div><p>In the Training-Model, the <a href="#pre-training-model">pre-training model</a> (including both <a href="#bert-base">Bert Base</a> and <a href="#barlow-head">Barlow Head</a>) is passed as a parameter. All model parameters are frozen, such that the training model will not update parameters from the pre-training model. During a forward pass, input is passed through the frozen pre-training model and then through an addition <a href="#ranking-head">Ranking Head</a> which is trained with the objective of optimising document ranking scores.</p>
<p>Moreover, the training-model includes a method <code>set_pretrain_model_to_eval</code> that is called from the main script. When the Training Model is set to train mode, we want to ensure that only the ranking head is actually in train mode, while all frozen pre-training sub-models remain in eval mode. Thus, <code>set_pretrain_model_to_eval</code> sets the <a href="#pre-training-model">pre-training model</a> back to eval.</p>

  
    <div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Computes top k accuracy</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">k_accuracy</span>(scores, answer_ids, target_answer_ids, k):
    <span style="color:#66d9ef">assert</span>  k <span style="color:#f92672">&gt;=</span> <span style="color:#ae81ff">1</span>
    <span style="color:#66d9ef">if</span>  k <span style="color:#f92672">==</span> <span style="color:#ae81ff">1</span>:
        prediction_indices <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>argmax(scores, dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
        prediction_answer_ids <span style="color:#f92672">=</span> answer_ids[prediction_indices]
        <span style="color:#66d9ef">return</span> (prediction_answer_ids <span style="color:#f92672">==</span> target_answer_ids)
    <span style="color:#66d9ef">elif</span>  k <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">1</span>:
        prediction_indices <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>topk(scores, k)<span style="color:#f92672">.</span>indices
        prediction_answer_ids <span style="color:#f92672">=</span> answer_ids[prediction_indices]
        <span style="color:#66d9ef">return</span> (prediction_answer_ids <span style="color:#f92672">==</span> target_answer_ids<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">1</span>))<span style="color:#f92672">.</span>any(<span style="color:#ae81ff">1</span>)

<span style="color:#75715e"># Computes top k average precision score</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">batch_map</span>(relevance, k):
    top_k <span style="color:#f92672">=</span> relevance[:, :k]
    batch_size <span style="color:#f92672">=</span> top_k<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">0</span>)
    pos <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">1</span>, top_k<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">1</span>) <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>, dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>float)<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">0</span>)<span style="color:#f92672">.</span>repeat(batch_size, <span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>to(top_k<span style="color:#f92672">.</span>device)
    csum, num_ans <span style="color:#f92672">=</span> top_k<span style="color:#f92672">.</span>cumsum(<span style="color:#ae81ff">1</span>), top_k<span style="color:#f92672">.</span>sum(<span style="color:#ae81ff">1</span>)
    apk <span style="color:#f92672">=</span> ((csum <span style="color:#f92672">/</span> pos <span style="color:#f92672">*</span> top_k)<span style="color:#f92672">.</span>sum(<span style="color:#ae81ff">1</span>) <span style="color:#f92672">/</span> num_ans)
    apk <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>masked_select(apk, <span style="color:#f92672">~</span>torch<span style="color:#f92672">.</span>isnan(apk))
    <span style="color:#66d9ef">return</span>  apk<span style="color:#f92672">.</span>sum()
</code></pre></div><p>We log 3 types of metrics: loss, accuracy and average precision (MAP).</p>
<ul>
<li>LOSS: we log both the validation and train loss, so that we can watch for signs of overfitting.</li>
<li>ACCURACY and MAP: When we say top <em>k</em> we measure a success criteria as each instance where  the correct answer id lies in the top <em>k</em> ranking.</li>
</ul>
<p>MAP penalises answers lower in the ranking while accuracy is blind to this.
We log these metrics with respect to questions only, conversations only, and both overall. We decided to log metrics seperately for the input types, as this project aims to increase the general performance of the model, meaning that one type should not be significantly worse than the other. Thus, it was important for us to track how our changes affected each individual type. Furthermore, we loG accuracy and MAP with k = 3, k = 10, as well as k = 1 for accuracy.</p>

  
    



  



<h1 id="timeline">Timeline</h1><p>The project went through 4 overall phases in regards to the construction of the model.</p>
<ol>
<li>Fundamental construction</li>
<li>Exploratory Data Analysis (EDA)</li>
<li>Generic improvements</li>
<li>Barlow Twins</li>
</ol>
<p>We will skip the majority of the details and focus on the key takeaways of each phase leading up to the final version of our model. This section will thus work as a brief summary of our work.</p>




  



<h2 id="1-fundamental-construction">1. Fundamental construction</h2><p>Without going into much detail, the first part of the project was the implementation, and understandment, of BERT. The two key decisions in this phase was freezing BERT and only using the CLS tokens. The decision to freeze BERT parameters was primarily taken to increase speed, which is crucial in the early stages of development, with a small sacrifice in accuracy. Unfreezing BERT would have likely increased performance, but for the sake of consistency, we kept it frozen. The second decision was using only the CLS tokens. As one knows, the CLS token is the classification tokens. In such, the CLS token can be said to encapsulate all the information of the sentences, a summary or topic so to say. We deemed this token very important when ranking and was therefore our main, and only, source of information. However, as we will discuss later, this can also come with consequences.</p>




  



<h2 id="2--exploratory-data-analysis">2.  Exploratory Data Analysis</h2><p>The input data contains 4 main pieces of information. The title, the content, the type (questions or conversations) and answer id (the &ldquo;true&rdquo; label). Each piece of information should be analysed due to its importance. Grouping title and content together, in a group that can be called text, we end up with 3 types of information that must be analyzed. The answer ids can tell us something about the grouping of information, the question type can tell the balance of a dataset and the text can show whether questions and conversations are seperated.</p>




  



<h3 id="21-answer-ids">2.1 Answer ids</h3><p>With some qualitative analysis, we investigated the assumption that input data which points to the same answer is about the same. This assumption is hard to fully investigate, but the crude investigated showed that the assumption is not completely off. However, to say that the assumption is true would be wrong. Based on this, we choose to apply the assumption, since it reduces the complexity of the problem and allows us to group the input data more easily.</p>




  



<h3 id="22-dataset-imbalance">2.2 Dataset imbalance</h3><p>The datasets (train and validation) are heavily skewed, having about 10 times the amount of conversations as opposed to questions. By including the generated questions, we get to a 2/3 split of conversations and questions, giving a more balanced dataset. Furthermore, we investigated casting short conversations as questions, as some conversations can be as short as a single sentence, esssentially making them a question. We chose to cast conversations with less than 100 characters into questions, making an almost perfectly balanced dataset. The balanced dataset is also crucial for the Barlow implementation later.</p>




  



<h3 id="23-pca">2.3 PCA</h3><p>In order to investigate whether questions and conversations, that has the same answer id, lie closely to eachother in latent space, we applied PCA to reduce the dimensionality down to 2D in order to visualize it. There was two main takeaways from the PCA plots. Firstly, there didn&rsquo;t appear to be distinct groupings based on answer ids and the embeddings was scattered in the latent space. There were certain areas with higher density of the same answer ids, but nothing that indicated seperation. Secondly, questions and conversations were not seperated and laid ontop of eachother. Howewer, conversatons were spread over the entire latent space, but questions appeared to be more clustered in certain areas. This might mean that BERT somewhat distinguishes between conversations and questions at a certain level.</p>




  



<h2 id="3-generic-improvements">3. Generic improvements</h2>



  



<h3 id="31-preprocessing">3.1 Preprocessing</h3><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">TFIDF_standard</span>(data):
    <span style="color:#75715e"># capture the title and content</span>
    title <span style="color:#f92672">=</span> [x[<span style="color:#e6db74">&#34;title&#34;</span>] <span style="color:#66d9ef">for</span> x <span style="color:#f92672">in</span> data]
    content <span style="color:#f92672">=</span> [x[<span style="color:#e6db74">&#34;content&#34;</span>] <span style="color:#66d9ef">for</span> x <span style="color:#f92672">in</span> data]

    <span style="color:#75715e"># split by words</span>
    vectorizer <span style="color:#f92672">=</span> TfidfVectorizer(token_pattern<span style="color:#f92672">=</span><span style="color:#e6db74">r</span><span style="color:#e6db74">&#39;(?u)\b\w+\b&#39;</span>)
    lookup <span style="color:#f92672">=</span> vectorizer<span style="color:#f92672">.</span>fit(content)<span style="color:#f92672">.</span>vocabulary_

    input_string <span style="color:#f92672">=</span> []
    word_split <span style="color:#f92672">=</span> re<span style="color:#f92672">.</span>compile(<span style="color:#e6db74">r</span><span style="color:#e6db74">&#39;(?u)\b\w+\b&#39;</span>)
    tokenizer <span style="color:#f92672">=</span> nltk<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>load(<span style="color:#e6db74">&#39;tokenizers/punkt/english.pickle&#39;</span>)

    <span style="color:#66d9ef">for</span> i, document <span style="color:#f92672">in</span> enumerate(content):
        <span style="color:#75715e"># split document into sentencces</span>
        sentences <span style="color:#f92672">=</span> tokenizer<span style="color:#f92672">.</span>tokenize(document)
        sentence_scores <span style="color:#f92672">=</span> []
        <span style="color:#66d9ef">for</span> sentence <span style="color:#f92672">in</span> sentences:
            score <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
            words <span style="color:#f92672">=</span> word_split<span style="color:#f92672">.</span>findall(sentence<span style="color:#f92672">.</span>lower())
            <span style="color:#75715e"># split sentence into words and compute the total tf-idf score per sentence</span>
            <span style="color:#66d9ef">for</span> word <span style="color:#f92672">in</span> words:
                score <span style="color:#f92672">+=</span> lookup[word]

            <span style="color:#75715e"># Relative score per word (incase of weird sentences (double periods) we have the if statement)</span>
            <span style="color:#66d9ef">if</span> len(words) <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span>: 
                score <span style="color:#f92672">/=</span> len(words)
            sentence_scores<span style="color:#f92672">.</span>append(score)

        <span style="color:#75715e"># Relative score per character for each sentence</span>
        sentence_scores <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(sentence_scores)
        <span style="color:#75715e"># Ranking of which sentence had the most important characters per length.</span>
        sorted_indices <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>argsort(sentence_scores)

        current_length <span style="color:#f92672">=</span> len(word_split<span style="color:#f92672">.</span>findall(title[i]))
        kept_indices <span style="color:#f92672">=</span> []

        <span style="color:#75715e"># Greedy Knapsack solution to add sentences per average char weight</span>
        <span style="color:#66d9ef">for</span> index <span style="color:#f92672">in</span> sorted_indices:
            word_count <span style="color:#f92672">=</span> word_split<span style="color:#f92672">.</span>findall(sentences[index])
            <span style="color:#66d9ef">if</span> current_length <span style="color:#f92672">&lt;</span> TFIDF_answer_length:
                kept_indices<span style="color:#f92672">.</span>append(index)
                current_length <span style="color:#f92672">+=</span> len(word_count)

        <span style="color:#75715e"># rearrange the sentences from (most -&gt; least important) to original order</span>
        final_string <span style="color:#f92672">=</span> title[i] <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34;.&#34;</span>
        <span style="color:#66d9ef">for</span> index <span style="color:#f92672">in</span> sorted(kept_indices):
            final_string <span style="color:#f92672">+=</span> <span style="color:#e6db74">&#34; &#34;</span> <span style="color:#f92672">+</span> sentences[index]
        input_string<span style="color:#f92672">.</span>append(final_string)
        
    <span style="color:#66d9ef">return</span>  input_string
</code></pre></div><p>As the input has to be of limited size, mainly due to speed and to reduce noise, we implement preprocessing to the input, so that we feed BERT with the, hopefully, most useful information. In order to do so, we applied TF-IDF in order to include the sentences with the most importance. Each sentence is scored by the relative score per word, which means summing each word&rsquo;s TF-IDF score and dividing by the number of words for the sentence. We then take the most important sentences, up to 128 tokens, and feed this to the model.</p>
<p>A potential improvement was to utilize the sections.json, so that there was an equal amount of sentences per section, however this implementation did not yield any significant improvements. Moreover, increasing the amount of tokens, from 128 up to 258 and 512, also did not yield any significant improvements. Instead it merely increased runtime by a significant factor (Twice the amount of data takes about twice the amount of time).</p>




  



<h3 id="32-dual-encoder">3.2 Dual encoder</h3><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span>  <span style="color:#a6e22e">train</span>(model, model_answers, optimizer, criterion, data_loader, answer_loader, cls_dictionary):
    answer_tokens <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>stack(list(cls_dictionary<span style="color:#f92672">.</span>values()))<span style="color:#f92672">.</span>to(device)
    answer_ids <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(list(cls_dictionary<span style="color:#f92672">.</span>keys()))<span style="color:#f92672">.</span>to(device)
    
    <span style="color:#75715e"># Metrics</span>
    total_loss <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
    
    model<span style="color:#f92672">.</span>train()
    model_answers<span style="color:#f92672">.</span>train()

    answer_iterator <span style="color:#f92672">=</span> iter(answer_loader)

    <span style="color:#66d9ef">for</span>  step, (data_batch) <span style="color:#f92672">in</span>  enumerate(data_loader):
        <span style="color:#75715e"># Keep updating the stale index (multiple updates for each ID each training step)</span>
        answer_batch <span style="color:#f92672">=</span> next(answer_iterator)

        <span style="color:#75715e"># clear previously calculated gradients</span>
        optimizer<span style="color:#f92672">.</span>zero_grad()

        <span style="color:#e6db74">&#39;&#39;&#39; Answer batch &#39;&#39;&#39;</span>
        answer_encoding, _, attention_mask, batch_ids <span style="color:#f92672">=</span> answer_batch
        new_answer_tokens  <span style="color:#f92672">=</span> model_answers(answer_encoding, attention_mask)

        <span style="color:#75715e"># Only update the relevant answer_tokens</span>
        answer_tokens[(answer_ids <span style="color:#f92672">==</span> batch_ids<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">1</span>))<span style="color:#f92672">.</span>nonzero()[:,<span style="color:#ae81ff">1</span>]] <span style="color:#f92672">=</span> new_answer_tokens

        <span style="color:#e6db74">&#39;&#39;&#39; Data batch &#39;&#39;&#39;</span>
        input_encoding, _, attention_mask, target_answer_ids, question_type <span style="color:#f92672">=</span> data_batch
        question_tokens <span style="color:#f92672">=</span> model(input_encoding, attention_mask)

        <span style="color:#75715e"># Calculate scores (batch_tokens vs stale_index_tokens)</span>
        scores <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>matmul(question_tokens, answer_tokens<span style="color:#f92672">.</span>T)
        target_answer_indices <span style="color:#f92672">=</span> (answer_ids <span style="color:#f92672">==</span> target_answer_ids<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">1</span>))<span style="color:#f92672">.</span>nonzero()[:, <span style="color:#ae81ff">1</span>]

        <span style="color:#e6db74">&#39;&#39;&#39; Compute loss &#39;&#39;&#39;</span>
        loss <span style="color:#f92672">=</span> criterion(scores, target_answer_indices)
        loss<span style="color:#f92672">.</span>backward()
        
        <span style="color:#75715e"># Gradient clipping</span>
        torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>utils<span style="color:#f92672">.</span>clip_grad_norm_(model<span style="color:#f92672">.</span>parameters(), <span style="color:#ae81ff">1</span>)
        
        optimizer<span style="color:#f92672">.</span>step()
        
        <span style="color:#75715e"># Important to detach from graph</span>
        answer_tokens<span style="color:#f92672">.</span>detach_()
        
        total_loss <span style="color:#f92672">+=</span> loss<span style="color:#f92672">.</span>item()
</code></pre></div><p>A significant improvement was seen when implementing a dual encoder setup. The setup uses two different heads, one for the input data and one for the answers. During a training step the model will update part of the stale index and this updated stale index is used to calculate the scores of the input data. In such, we collect gradients from both the indexing of answers and the scoring of questions and conversations. There is still only calculate a single loss based on the scoring, but this loss is backpropagated into two seperate heads now.</p>
<p>This implementation had a more gradual learning curve, starting off worse than the other models, but was also to train for longer (10-20 epochs before stagnating) and giving significant improvements on all metrics. Most noticably, the dual encoder implementation surpasses the other models before the stagnate. This means that the dual encoders increased performance is not just due to the longer training. Stopping the dual encoder model at the same point as the others would have also yielded significant improvements.</p>




  



<h3 id="33-gradient-clipping-l2-regularization--batch-normalization">3.3 Gradient clipping, L2 regularization &amp; batch normalization</h3><p>We apply some of the common deep learning improvements, such as gradient clipping, l2 regularization and batch normalization. Without much investigation in these, we saw immediate improvements. These techniques help us stabilize the learning and prevent overfitting.</p>




  



<h2 id="4-barlow-twins">4. Barlow Twins</h2>
  
    



  



<h1 id="discussion">Discussion</h1><aside class="notice">
Nat
</aside>
<p>We can see that Barlow Head accomplishes what it needs to do. (it works) but still that it breaks the downstream BERT task. We tried both frozen and unfrozen heads, and in both cases BERT is much worse off with the pretraining than with random initialized weights. We made sure it can&rsquo;t be a scaling issue as everything is normalized.</p>
<p>Take-away, BERT makes its own representations in the encoding space that work well for document ranking, and tweaking these seems to hurt the model. Maybe moving questions and conversations together is an idea that makes sense from a human perspective, but interferes with the models deeper understanding of the datatypes. We don&rsquo;t deeply understand how the model represents the data.</p>
<aside class="notice">
Mikkel
</aside>
<p>Seeing as project did not yield any improvements and instead significantly worsen it, the question to ask is why. Why does BERT and Barlow work seperately, but completely fail when coupled.</p>
<p>One of the reasons can be the assumption that questions and conversations with the same <code>answer id</code> should lie in the same latent space. This might be a very human intuition that does not necessarily translate into computer understanding. Moreover, our decision to only include the CLS token might also influence this. The CLS token is the classification token and BERT&rsquo;s understanding of classification might differ from humans. Even though a topic may be the same for a question and a conversation, the language in short and concise text (questions) might put it into a different classification than noisy and long text (conversations).</p>
<p>Secondly, we notice that the pretrained model is more or less broken for the ranking task. The model totally fails and has an accuracy slightly above 0%, much less than BERT achieves. This hints to the fact that there might be a fundamental problem, not just that Barlow might be a wrong assumption. Raffle.ai has had previous experience with BERT&rsquo;s apparent resistance to pretraining tasks. The underlying problem with pretraining might be that it interfers too much with the fundamental understandment of language by BERT. Additionally, there might be a need for special integration, rather than a trained head put ontop of BERT. However, these considerations requires much energy to investigate and we leave this for the future work.</p>

  

      </div>
      <div class="dark-box">
        
          <div class="lang-selector" x-data="langController" x-init="initLangs([{&#34;key&#34;:&#34;python&#34;,&#34;name&#34;:&#34;Python&#34;}])">
            <template x-for="(tab, index) in tabs">
              <a x-text="tab.name" :class="{ 'active': tab.active }" @click="changeLanguage(index)"></a>
            </template>
          </div>
        
      </div>
    </div>
    
  </body>
</html>
