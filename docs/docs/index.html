<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta content="IE=edge,chrome=1" http-equiv="X-UA-Compatible">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <title>
      Docs
    </title>
    








<link href='/DocRankingWebsite/styles/screen.min.dbadab296f1baed94fc5109ecef71e620384615bbb9c2c5fb0f0c9f10f900ab9.css' rel="stylesheet" media="screen" integrity="sha256-262rKW8brtlPxRCezvceYgOEYVu7nCxfsPDJ8Q&#43;QCrk=" />
<link href='/DocRankingWebsite/styles/print.min.e0db9e70d333a92515c28264c527870199194686a03c044b826e9827ec960b9d.css' rel="stylesheet" media="print" integrity="sha256-4NuecNMzqSUVwoJkxSeHAZkZRoagPARLgm6YJ&#43;yWC50=" />


    




  

<script src="/DocRankingWebsite/js/index.e4619e460f1bf032b63c9e8bf4c022ec118c03648d58c61e6a8402ae9e194017.js" integrity="sha256-5GGeRg8b8DK2PJ6L9MAi7BGMA2SNWMYeaoQCrp4ZQBc=" defer></script>

    


    <style>
      [x-cloak] {
      display: none !important;
      }
    </style>
  </head>
  <body class="index" x-data="{ navOpen: false }" x-cloak>
    <a href="#" id="nav-button" @click="navOpen = !navOpen" :class="{'open': navOpen }">
      <span>
        NAV
        <img src='/DocRankingWebsite/images/navbar.png'/>
      </span>
    </a>
    <div class="toc-wrapper" :class="{'open': navOpen }">
      
       <img src='/DocRankingWebsite/images/logo.png' class="logo" />
      
        <div class="search" x-data="searchController">
          <input x-model.debounce.100ms="query" type="search" class="search" id="input-search" placeholder='Search'>
          <ul class="search-results visible" x-show="results.length > 0" x-transition.duration.700ms >
            <template x-for="item in results">
              <li>
                <a x-text="item.title"></a>
              </li>
            </template>
          </ul>
        </div>
      
      
  
  
    
  
  <ul id="toc" class="toc-list-h1" x-data="tocController" x-init="load([{&#34;id&#34;:&#34;intro&#34;,&#34;level&#34;:1,&#34;sub&#34;:[{&#34;id&#34;:&#34;about&#34;,&#34;level&#34;:2,&#34;title&#34;:&#34;About&#34;},{&#34;id&#34;:&#34;motivation&#34;,&#34;level&#34;:2,&#34;sub&#34;:[],&#34;title&#34;:&#34;Motivation&#34;}],&#34;title&#34;:&#34;Intro&#34;},{&#34;id&#34;:&#34;metrics&#34;,&#34;level&#34;:1,&#34;sub&#34;:[],&#34;title&#34;:&#34;Metrics&#34;},{&#34;id&#34;:&#34;timeline&#34;,&#34;level&#34;:1,&#34;sub&#34;:[{&#34;id&#34;:&#34;exploratory-data-analysis&#34;,&#34;level&#34;:2,&#34;sub&#34;:[{&#34;id&#34;:&#34;answer-ids&#34;,&#34;level&#34;:3,&#34;title&#34;:&#34;Answer ids&#34;},{&#34;id&#34;:&#34;dataset-imbalance&#34;,&#34;level&#34;:3,&#34;title&#34;:&#34;Dataset imbalance&#34;},{&#34;id&#34;:&#34;pca&#34;,&#34;level&#34;:3,&#34;title&#34;:&#34;PCA&#34;}],&#34;title&#34;:&#34;Exploratory Data Analysis&#34;},{&#34;id&#34;:&#34;document-ranking-model&#34;,&#34;level&#34;:2,&#34;sub&#34;:[{&#34;id&#34;:&#34;preprocessing-for-bert&#34;,&#34;level&#34;:3,&#34;title&#34;:&#34;Preprocessing for BERT&#34;},{&#34;id&#34;:&#34;bert-with-one-head&#34;,&#34;level&#34;:3,&#34;title&#34;:&#34;BERT with one head&#34;},{&#34;id&#34;:&#34;bert-with-dual-encoder&#34;,&#34;level&#34;:3,&#34;title&#34;:&#34;BERT with Dual Encoder&#34;},{&#34;id&#34;:&#34;gradient-clipping-l2-regularization--batch-normalization&#34;,&#34;level&#34;:3,&#34;title&#34;:&#34;Gradient clipping, L2 regularization \u0026amp; batch normalization&#34;},{&#34;id&#34;:&#34;hyperparameter-tuning&#34;,&#34;level&#34;:3,&#34;title&#34;:&#34;Hyperparameter Tuning&#34;}],&#34;title&#34;:&#34;Document Ranking Model&#34;},{&#34;id&#34;:&#34;pretraining-with-barlow-twins&#34;,&#34;level&#34;:2,&#34;sub&#34;:[{&#34;id&#34;:&#34;theory&#34;,&#34;level&#34;:3,&#34;title&#34;:&#34;Theory&#34;},{&#34;id&#34;:&#34;code&#34;,&#34;level&#34;:3,&#34;title&#34;:&#34;Code&#34;},{&#34;id&#34;:&#34;implementation&#34;,&#34;level&#34;:3,&#34;title&#34;:&#34;Implementation&#34;}],&#34;title&#34;:&#34;Pretraining with Barlow Twins&#34;}],&#34;title&#34;:&#34;Timeline&#34;},{&#34;id&#34;:&#34;model&#34;,&#34;level&#34;:1,&#34;sub&#34;:[{&#34;id&#34;:&#34;bert-base&#34;,&#34;level&#34;:2,&#34;title&#34;:&#34;BERT Base&#34;},{&#34;id&#34;:&#34;barlow-head&#34;,&#34;level&#34;:2,&#34;title&#34;:&#34;Barlow Head&#34;},{&#34;id&#34;:&#34;ranking-head&#34;,&#34;level&#34;:2,&#34;title&#34;:&#34;Ranking Head&#34;},{&#34;id&#34;:&#34;pre-training-model&#34;,&#34;level&#34;:2,&#34;title&#34;:&#34;Pre-training Model&#34;},{&#34;id&#34;:&#34;training-model&#34;,&#34;level&#34;:2,&#34;sub&#34;:[],&#34;title&#34;:&#34;Training Model&#34;}],&#34;title&#34;:&#34;Model&#34;},{&#34;id&#34;:&#34;discussion&#34;,&#34;level&#34;:1,&#34;sub&#34;:[{&#34;sub&#34;:[]}],&#34;title&#34;:&#34;Discussion&#34;}])" @scroll.window="onScroll()">
    
    <template x-for="row in rows">
      <li>
        <a x-text="row.title" @click="click(row)" :href="`#${row.id}`" class="toc-link" x-bind="rowClass(row)"></a>
        
          <ul x-show="row.open" x-bind="transitions()" class="da-toc-list-h2">
            <template x-for="row in row.sub">
              <li>
                <a x-text="row.title" @click="click(row)" :href="`#${row.id}`" class="toc-link" x-bind="rowClass(row)"></a>
                
                  <ul x-show="row.open" x-bind="transitions()" class="toc-list-h3">
                    <template x-for="row in row.sub">
                      <li>
                        <a  x-text="row.title" @click="click(row)" :href="`#${row.id}`"  class="toc-link" x-bind="rowClass(row)"></a>
                      </li>
                    </template>
                  </ul>
                
              </li>
            </template>
          </ul>
        
      </li>
    </template>
  </ul>

      
      
      

    </div>
    <div class="page-wrapper">
      <div class="dark-box"></div>
      <div class="content">
        
  
    
  
    
  
    
  
    
  
    
  
  
    





<h1 id="intro">Intro</h1>



  



<h2 id="about">About</h2><p>This website is a Handover Document by <a href="https://github.com/mikkelfo">Mikkel Odgaard</a> and <a href="https://github.com/nklingen">Natasha Klingenbrunn</a> for a collaboration project between Denmark&rsquo;s Technical University and <a href="https://www.raffle.ai">Raffle.ai</a>. The <a href="https://github.com/nklingen/BERT_Question_Answering">final code</a> can be found on Github.</p>
<p>The website is broken down as follows:</p>
<ol>
<li>A brief conceptual background is given for Barlow Twins and BERT.</li>
<li>Next, the Timeline section will discuss the evolution of our project and discuss the rationale behind given design choices.</li>
<li>The code break-down for our model and barlow loss can be found in their respective sections.</li>
<li>Finally, the discussion section will outline why believe our model was not successful, and touch upon our final thoughts.</li>
</ol>




  



<h2 id="motivation">Motivation</h2><p>Raffle.ai&rsquo;s BERT Document Ranking Model sees better performance on questions than on conversations. The motivation behind this project is to leverage SSL to yield a more constant performance across input types. To accomplish this, we will introduce a pre-training step implementing Barlow Twins to push encodings together for similar questions and conversations. Having a successful implementation of the pre-training model will allow us to analyse the downstream effects in the BERT Ranking Model.</p>
<p>For example, one hypothesis prior to beginning this project was that the BERT Ranking Model might project question and conversations into two seperate planes. Thus, while it may be very successful in one input-type, this does not translate to high performance for the other. We hope to learn whether having more similar embeddings for questions and conversations has a positive effect on the model, or whether the model&rsquo;s own learned representations are better. While we didn&rsquo;t find it likely to have a better initial performance (if projecting all inputs from the same answer_id together had a lower loss in the downstream task, it would have done it anyways), but hoped that we could see other indicators of pre-training benefitting the model, such as a quicker rate of converging, for example.</p>

  
    



  



<h1 id="metrics">Metrics</h1><blockquote>
<p>To compute top k accuracy</p>
</blockquote>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">k_accuracy</span>(scores, answer_ids, target_answer_ids, k):
    <span style="color:#66d9ef">assert</span>  k <span style="color:#f92672">&gt;=</span> <span style="color:#ae81ff">1</span>
    <span style="color:#66d9ef">if</span>  k <span style="color:#f92672">==</span> <span style="color:#ae81ff">1</span>:
        prediction_indices <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>argmax(scores, dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
        prediction_answer_ids <span style="color:#f92672">=</span> answer_ids[prediction_indices]
        <span style="color:#66d9ef">return</span> (prediction_answer_ids <span style="color:#f92672">==</span> target_answer_ids)
    <span style="color:#66d9ef">elif</span>  k <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">1</span>:
        prediction_indices <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>topk(scores, k)<span style="color:#f92672">.</span>indices
        prediction_answer_ids <span style="color:#f92672">=</span> answer_ids[prediction_indices]
        <span style="color:#66d9ef">return</span> (prediction_answer_ids <span style="color:#f92672">==</span> target_answer_ids<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">1</span>))<span style="color:#f92672">.</span>any(<span style="color:#ae81ff">1</span>)
</code></pre></div><blockquote>
<p>To compute top k average precision score (from Raffle&rsquo;s codebase)</p>
</blockquote>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">batch_map</span>(relevance, k):
    top_k <span style="color:#f92672">=</span> relevance[:, :k]
    batch_size <span style="color:#f92672">=</span> top_k<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">0</span>)
    pos <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">1</span>, top_k<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">1</span>) <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>, dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>float)<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">0</span>)<span style="color:#f92672">.</span>repeat(batch_size, <span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>to(top_k<span style="color:#f92672">.</span>device)
    csum, num_ans <span style="color:#f92672">=</span> top_k<span style="color:#f92672">.</span>cumsum(<span style="color:#ae81ff">1</span>), top_k<span style="color:#f92672">.</span>sum(<span style="color:#ae81ff">1</span>)
    apk <span style="color:#f92672">=</span> ((csum <span style="color:#f92672">/</span> pos <span style="color:#f92672">*</span> top_k)<span style="color:#f92672">.</span>sum(<span style="color:#ae81ff">1</span>) <span style="color:#f92672">/</span> num_ans)
    apk <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>masked_select(apk, <span style="color:#f92672">~</span>torch<span style="color:#f92672">.</span>isnan(apk))
    <span style="color:#66d9ef">return</span>  apk<span style="color:#f92672">.</span>sum()
</code></pre></div><p>We log 3 types of metrics: loss, accuracy and average precision (MAP).</p>
<aside class="notice">
When we say top *k* we measure a success criteria as each instance where the correct answer id lies in the *k* top scores. Whereas accuracy is blind to how high the relative score was, as long as it still falls in the top k, MAP will still (slightly) penalize correct answers that weren't ranked highly enough. 
</aside>
<ul>
<li>Loss
<ul>
<li>Validation Loss</li>
<li>Train loss</li>
</ul>
</li>
<li>Accuracy
<ul>
<li>Questions
<ul>
<li>k = 1, k = 3, k = 10</li>
</ul>
</li>
<li>Conversations
<ul>
<li>k = 1, k = 3, k = 10</li>
</ul>
</li>
<li>Overall
<ul>
<li>k = 1, k = 3, k = 10</li>
</ul>
</li>
</ul>
</li>
<li>MAP
<ul>
<li>Questions
<ul>
<li>k = 3, k = 10</li>
</ul>
</li>
<li>Conversations
<ul>
<li>k = 3, k = 10</li>
</ul>
</li>
<li>Overall
<ul>
<li>k = 3, k = 10</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>As our project aims to increase the general performance of the model, this means that we want to identify if one query type is significantly worse than the other. Thus, it was important for us to track how our changes to the model affected each of the query types. Moreover, we specifically looked at the accuracy top 3 metrics to compare with Raffle&rsquo;s own model. Lastly, we used the loss metrics to monitor for signs of overfitting.</p>

  
    



  



<h1 id="timeline">Timeline</h1><p>This section will be a general summary of our work and the evolution of the project. The project went through 3 overall phases in regards to the construction of the model.</p>
<ol>
<li>Exploratory Data Analysis (EDA)</li>
<li>Document Ranking Model</li>
<li>Barlow Twins</li>
</ol>




  



<h2 id="exploratory-data-analysis">Exploratory Data Analysis</h2><p>The input data contains 4 main pieces of information. The title, the content, the type (questions or conversations) and answer id (the &ldquo;true&rdquo; label). Each piece of information should be analysed due to its importance. Grouping title and content together, in a group that can be called text, we end up with 3 types of information that must be analyzed. The answer ids can tell us something about the grouping of information, the question type can tell the balance of a dataset and the text can show whether questions and conversations are seperated.</p>




  



<h3 id="answer-ids">Answer ids</h3><p>With some qualitative analysis, we investigated the assumption that input data which points to the same answer is about the same. This assumption is hard to fully investigate, but the crude investigated showed that the assumption is not completely off. However, to say that the assumption is true would be wrong. Based on this, we choose to apply the assumption, since it reduces the complexity of the problem and allows us to group the input data more easily.</p>




  



<h3 id="dataset-imbalance">Dataset imbalance</h3><p>The datasets (train and validation) are heavily skewed, having about 10 times the amount of conversations as opposed to questions. By including the generated questions, we get to a 2/3 split of conversations and questions, giving a more balanced dataset. Furthermore, we investigated casting short conversations as questions, as some conversations can be as short as a single sentence, esssentially making them a question. We chose to cast conversations with less than 100 characters into questions, making an almost perfectly balanced dataset. The balanced dataset is also crucial for the Barlow implementation later.</p>




  



<h3 id="pca">PCA</h3><p>In order to investigate whether questions and conversations, that has the same answer id, lie closely to eachother in latent space, we applied PCA to reduce the dimensionality down to 2D in order to visualize it. There was two main takeaways from the PCA plots. Firstly, there didn&rsquo;t appear to be distinct groupings based on answer ids and the embeddings was scattered in the latent space. There were certain areas with higher density of the same answer ids, but nothing that indicated seperation. Secondly, questions and conversations were not seperated and laid ontop of eachother. Howewer, conversatons were spread over the entire latent space, but questions appeared to be more clustered in certain areas. This might mean that BERT somewhat distinguishes between conversations and questions at a certain level.</p>




  



<h2 id="document-ranking-model">Document Ranking Model</h2><p>The first part of the project was to implement and understand a BERT model trained for Document Ranking.</p>




  



<h3 id="preprocessing-for-bert">Preprocessing for BERT</h3><blockquote>
<p>Step 1. Compute the TF-IDF score for each word in the corpus</p>
</blockquote>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">TFIDF_standard</span>(data):
	title <span style="color:#f92672">=</span> [x[<span style="color:#e6db74">&#34;title&#34;</span>] <span style="color:#66d9ef">for</span> x <span style="color:#f92672">in</span> data]
	content <span style="color:#f92672">=</span> [x[<span style="color:#e6db74">&#34;content&#34;</span>] <span style="color:#66d9ef">for</span> x <span style="color:#f92672">in</span> data]
	vectorizer <span style="color:#f92672">=</span> TfidfVectorizer(token_pattern<span style="color:#f92672">=</span><span style="color:#e6db74">r</span><span style="color:#e6db74">&#39;(?u)\b\w+\b&#39;</span>)
	lookup <span style="color:#f92672">=</span> vectorizer<span style="color:#f92672">.</span>fit(content)<span style="color:#f92672">.</span>vocabulary_
    input_string, kept_indices <span style="color:#f92672">=</span> []
</code></pre></div><blockquote>
<p>Step 2. Compute the relative TF-IDF score for each sentence</p>
</blockquote>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">	word_split <span style="color:#f92672">=</span> re<span style="color:#f92672">.</span>compile(<span style="color:#e6db74">r</span><span style="color:#e6db74">&#39;(?u)\b\w+\b&#39;</span>)
	tokenizer <span style="color:#f92672">=</span> nltk<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>load(<span style="color:#e6db74">&#39;tokenizers/punkt/english.pickle&#39;</span>)
	<span style="color:#66d9ef">for</span> i, document <span style="color:#f92672">in</span> enumerate(content):
		sentence_scores <span style="color:#f92672">=</span> []
		<span style="color:#66d9ef">for</span> sentence <span style="color:#f92672">in</span> tokenizer<span style="color:#f92672">.</span>tokenize(document):
			score <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
			<span style="color:#66d9ef">for</span> word <span style="color:#f92672">in</span> word_split<span style="color:#f92672">.</span>findall(sentence<span style="color:#f92672">.</span>lower())
				score <span style="color:#f92672">+=</span> lookup[word]
			num_words <span style="color:#f92672">=</span> max(<span style="color:#ae81ff">1</span>, len(words))
			sentence_scores<span style="color:#f92672">.</span>append(score<span style="color:#f92672">/</span>num_words) 
</code></pre></div><blockquote>
<p>Step 3. Order each sentence from most to least important based on relative TF-IDF score</p>
</blockquote>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">		sentence_scores <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(sentence_scores)
		sorted_indices <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>argsort(sentence_scores)
</code></pre></div><blockquote>
<p>Step 4. Take the max number of sentence (from most important to least) you can fit in 128 tokens</p>
</blockquote>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">        current_length <span style="color:#f92672">=</span> len(word_split<span style="color:#f92672">.</span>findall(title[i])) <span style="color:#75715e"># title is always returned</span>
		<span style="color:#75715e"># Greedy Knapsack solution to add sentences per average char weight</span>
		<span style="color:#66d9ef">for</span> index <span style="color:#f92672">in</span> sorted_indices:
			word_count <span style="color:#f92672">=</span> word_split<span style="color:#f92672">.</span>findall(sentences[index])
			<span style="color:#66d9ef">if</span> current_length <span style="color:#f92672">&lt;</span> TFIDF_answer_length:
				kept_indices<span style="color:#f92672">.</span>append(index)
				current_length <span style="color:#f92672">+=</span> len(word_count)
</code></pre></div><blockquote>
<p>Step 5. Re-order the sentences to the original order</p>
</blockquote>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">		final_string <span style="color:#f92672">=</span> title[i] <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34;.&#34;</span>
		<span style="color:#66d9ef">for</span> index <span style="color:#f92672">in</span> sorted(kept_indices):
			final_string <span style="color:#f92672">+=</span> <span style="color:#e6db74">&#34; &#34;</span> <span style="color:#f92672">+</span> sentences[index]
		input_string<span style="color:#f92672">.</span>append(final_string)
		
	<span style="color:#66d9ef">return</span>  input_string
</code></pre></div><p>As the input to BERT has to be of limited size due to time and memory constraints, we preprocess the input, to remove noise in the data and feed BERT with only the most useful information.</p>
<p>To do so, We decided to include the sentences with the highest importance according to relative sentence TF-IDF score. We computed this in the following way.</p>
<ol>
<li>Compute the TF-IDF score for each word in the corpus</li>
<li>Compute the relative TF-IDF score for each sentence
<ul>
<li>First sum up the total TF-IDF score for each word in the sentence</li>
<li>Divide by the amount of words in the sentence to compute a relative score</li>
</ul>
</li>
<li>Order each sentence from most to least important based on relative TF-IDF score</li>
<li>Take the max number of sentence (from most important to least) you can fit in 128 tokens</li>
<li>Re-order the sentences to the original order</li>
</ol>
<p>The entire code is demonstrated on the right.</p>
<p>Additionally, we tried utilizing the sections.json file, and instead return an equal number of top TF-IDF scored sentences <em>per section</em>, however this implementation did not yield any significant improvements.</p>
<p>Moreover, we also attempted to increase the amount of tokens from 128 to 258 and 512; this did not yield any significant improvements. Instead it merely increased runtime lineraly in proportion to the number of tokens. (Twice the amount of data took roughly twice the amount of time).</p>




  



<h3 id="bert-with-one-head">BERT with one head</h3><aside class="notice">
For clarity, question and conversations embeddings will be called "queries".
</aside>
<p>The model was implemented as follows:</p>
<ol>
<li>Compute the answer Database
<ul>
<li>We passed the answers through a BERT model with a head to get a database of answer encodings.</li>
</ul>
</li>
<li>Compute the query embeddings
<ul>
<li>Next, we passed the questions and conversations through the same BERT model with the same head to get their encodings.</li>
</ul>
</li>
</ol>
<p><img src="https://nklingen.github.io/DocRankingWebsite//images/BERT1.png" alt="BERT 1"></p>
<ol start="3">
<li>Document Ranking
<ul>
<li>To compute most related documents for a given query, we compute the cross product between the query embeddings and the entire answer database.</li>
<li>We then find the argmax for each query, that is the answer document that had the highest score for that query.</li>
<li>We return the index of the argmax.</li>
</ul>
</li>
</ol>
<p><img src="https://nklingen.github.io/DocRankingWebsite//images/BERT2.png" alt="BERT 2"></p>
<p>This is a simplified explanation for the instance where k=1. To compute the top k most relevant documents, the indices of the top k argmax elements are returned for each query.</p>
<p>We will also discuss some design choices in implementing BERT. Namely, the two key decisions in this phase were <strong>freezing BERT</strong> and <strong>only using the CLS tokens</strong>.</p>
<ul>
<li>The decision to freeze BERT parameters was primarily taken to increase speed and avoid running into issues of memory constraint, which was crutial for us given this was a short-term research project and we had GPU resource constraints. We knew this would entail a decrease in accuracy. Unfreezing BERT and adapting the model for all experiments would have likely increased performance.</li>
<li>The second decision was using only the CLS tokens. Briefly, the CLS token is first column of the BERT output, typically used for classification tasks. The CLS token can be said to encapsulate all the information of the input, to give a sort of general summary. We deemed this token most important for ranking. Thus, given the same speed and memory constraints, we chose to only use the CLS token and discard the rest of the BERT output. However, as we will discuss later, this can also come with consequences.</li>
</ul>




  



<h3 id="bert-with-dual-encoder">BERT with Dual Encoder</h3><blockquote>
<p>Using the Dual encoder, we want to set both models to train</p>
</blockquote>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span>  <span style="color:#a6e22e">train</span>(model, model_answers, optimizer, criterion, data_loader, answer_loader, cls_dictionary):

    answer_tokens <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>stack(list(cls_dictionary<span style="color:#f92672">.</span>values()))<span style="color:#f92672">.</span>to(device)
    answer_ids <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(list(cls_dictionary<span style="color:#f92672">.</span>keys()))<span style="color:#f92672">.</span>to(device)
    total_loss <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
    model<span style="color:#f92672">.</span>train() <span style="color:#75715e"># Query encodings</span>
    model_answers<span style="color:#f92672">.</span>train() <span style="color:#75715e"># Answer DB encodings</span>
</code></pre></div><blockquote>
<p>For each new batch from the queries, we first update the stale index over a batch of answers</p>
</blockquote>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">    answer_iterator <span style="color:#f92672">=</span> iter(answer_loader)
    <span style="color:#66d9ef">for</span>  step, (data_batch) <span style="color:#f92672">in</span>  enumerate(data_loader):
        answer_batch <span style="color:#f92672">=</span> next(answer_iterator)
        optimizer<span style="color:#f92672">.</span>zero_grad()
        answer_encoding, _, attention_mask, batch_ids <span style="color:#f92672">=</span> answer_batch
        new_answer_tokens  <span style="color:#f92672">=</span> model_answers(answer_encoding, attention_mask)
        answer_tokens[(answer_ids <span style="color:#f92672">==</span> batch_ids<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">1</span>))<span style="color:#f92672">.</span>nonzero()[:,<span style="color:#ae81ff">1</span>]] <span style="color:#f92672">=</span> new_answer_tokens
</code></pre></div><blockquote>
<p>We then pass the queries though their respective model to compute their encoding</p>
</blockquote>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">        input_encoding, _, attention_mask, target_answer_ids, question_type <span style="color:#f92672">=</span> data_batch
        question_tokens <span style="color:#f92672">=</span> model(input_encoding, attention_mask)
</code></pre></div><blockquote>
<p>Finally we compute the score between the encodings and the answer database</p>
</blockquote>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">        scores <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>matmul(question_tokens, answer_tokens<span style="color:#f92672">.</span>T)
        target_answer_indices <span style="color:#f92672">=</span> (answer_ids <span style="color:#f92672">==</span> target_answer_ids<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">1</span>))<span style="color:#f92672">.</span>nonzero()[:, <span style="color:#ae81ff">1</span>]
        loss <span style="color:#f92672">=</span> criterion(scores, target_answer_indices) <span style="color:#75715e"># compute loss </span>
        loss<span style="color:#f92672">.</span>backward() <span style="color:#75715e"># compute gradients</span>
        torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>utils<span style="color:#f92672">.</span>clip_grad_norm_(model<span style="color:#f92672">.</span>parameters(), <span style="color:#ae81ff">1</span>) <span style="color:#75715e"># gradient clipping </span>
        optimizer<span style="color:#f92672">.</span>step() <span style="color:#75715e"># update weights</span>
        answer_tokens<span style="color:#f92672">.</span>detach_()
        total_loss <span style="color:#f92672">+=</span> loss<span style="color:#f92672">.</span>item()
</code></pre></div><p>On a tip from Raffle, we implemented a Dual Encoder, wherein the create two heads, one for the query data and one for the answers. Thus the two inputs are no longer encoded in the same model, and each encoder can specialise on its respective input.</p>
<p>During a training step the model will update part of the stale index, as seen in the code to the right. This updated stale index is used to score the given document for new incoming queries. In such, we collect gradients from both the indexing of answers and the scoring of questions and conversations (but later detach the model for the answers). We only calculate a single loss based on the scoring, and then backpropagate this loss onto the two seperate heads.</p>
<p><img src="https://nklingen.github.io/DocRankingWebsite//images/BERT3.png" alt="BERT 3"></p>




  



<h3 id="gradient-clipping-l2-regularization--batch-normalization">Gradient clipping, L2 regularization &amp; batch normalization</h3><p>We apply some of the common deep learning improvements, such as gradient clipping, and l2 regularization, and saw immediate improvements. These techniques help us stabilize the learning and prevent overfitting.</p>
<p>Later, we also exchanged the gradient clipping after the model terminates with batch normalization in between the blocks of the model.</p>




  



<h3 id="hyperparameter-tuning">Hyperparameter Tuning</h3><p>As the graphics are a bit small, the entire Hyperparameter Tuning Sweep can be found <a href="https://wandb.ai/nklingen/BERT_Question_Answering/reports/Hyperparameter-Tuning-Baseline--VmlldzoxMTQ0OTE5">here</a>.</p>
<p>In order to fix a model for our expeirments we found a set of parameters using hyperparameter tuning. We tune with respect to the validation loss over 4 parameters: dropout rate, learning rate, batch size and weight decay. We utilized &lsquo;Weights &amp; Biases&rsquo; automatic hyperparameter tuning (sweeps) and chose to us Bayes optimisation to optimise the search. We do the sweep on the <em>Dual Encoder</em> model described above.</p>
<p><img src="https://nklingen.github.io/DocRankingWebsite//images/sweep.jpg" alt="Overview of all the runs"></p>
<p>We fix the <code>max_length=128</code>, i.e. the amount of tokens fed to BERT, and the max number of <code>epochs=50</code>. The sweep finds the following parameters:</p>
<p><code>batch size=50</code>
<code>dropout=0.11</code>
<code>learning rate=1.4e-4</code>
<code>weight decay=9.3e-4</code></p>
<p>Moreover, &lsquo;Weights &amp; Biases&rsquo; tells us the importance and correlation of each parameter. It clearly shows the learning rate being the most influential, which is expected due to its large impact on training.</p>
<p><img src="https://nklingen.github.io/DocRankingWebsite//images/param_importance.png" alt="Parameter importance"></p>




  



<h2 id="pretraining-with-barlow-twins">Pretraining with Barlow Twins</h2><p>At this stage, we had a very basic, working Document Ranking Model. We now wanted to try out our hypothesis, that pushing together similar question and conversation encodings might improve the model.</p>




  



<h3 id="theory">Theory</h3><p>Barlow Twins takes a batch of samples, applies noise to generate two distored versions, then passes both versions through two identical networks to get their corresponding embeddings. The Barlow loss is then computed on the embeddings, wherein the goal is to get the cross-correlation matrix between the embeddings as close as possible to the identity matrix. In this way, the embeddings of the two versions of the sample are encourraged to be similar, while redundancy between the components of the vectors is penalized.</p>
<p><em>&ldquo;Barlow Twins is competitive with state-of-the-art methods for self-supervised learning while being conceptually simpler, naturally avoiding trivial constant (i.e. collapsed) embeddings, and being robust to the training batch size.&quot;</em> From: <a href="https://arxiv.org/pdf/2103.03230.pdf">Barlow Twins: Self-Supervised Learning via Redundancy Reduction</a></p>
<p>In our project, we implement Barlow Twins to encourage similar embeddings between questions and conversations with the same <code>answer_id</code>. The assumption here is that questions and conversations that have the same answer must be related in some capacity, and moreover, that they must share an underlying concept. Hereby, the two &ldquo;versions&rdquo; can be likened to distortion of the same underlying concept.</p>
<p>Whereas the original Barlow Paper takes a batch of samples and applies noise, in our implementation we already have the distorted matrices. For a given <code>answer_id</code>, this is simply a batch with associated questions (f), and a batch with associated conversations (g). In our dataloader, we ensure the batches are of equal size.</p>
<p><img src="https://nklingen.github.io/DocRankingWebsite//images/Barlow_1.png" alt="Barlow 1"></p>
<p>Now we compute the correlation between f and g.
<img src="https://nklingen.github.io/DocRankingWebsite//images/Barlow_eq.png" alt="Barlow EQ"></p>
<p>This yields the following correlation matrix:
<img src="https://nklingen.github.io/DocRankingWebsite//images/Barlow_matrix.png" alt="Barlow Matrix"></p>
<p>Having now computed the correlation matrix, we want to encourage it to resemble the identity matrix. Hereby, we have two terms. In the <code>invariance_term</code> we encourage the diagonals (marked in grey) to be close to 1 and hereby for the model to be distortion agnostic, while in the <code>redundancy_reduction_term</code> we encourage all off-diagonals to be close to 0.</p>
<p><img src="https://nklingen.github.io/DocRankingWebsite//images/Barlow_3.png" alt="Barlow 3"></p>




  



<h3 id="code">Code</h3><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">
<span style="color:#66d9ef">for</span> batch <span style="color:#f92672">in</span> B:
    <span style="color:#f92672">...</span> 
    input_encoding_ques, input_encoding_conv, _, _, attention_mask_conv, attention_mask_ques <span style="color:#f92672">=</span> batch
    barlow_sample_batch_size <span style="color:#f92672">=</span> input_encoding_ques<span style="color:#f92672">.</span>squeeze()<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]

    <span style="color:#75715e"># question (f) and conversation (g) encodings</span>
    f <span style="color:#f92672">=</span> model(input_encoding_ques<span style="color:#f92672">.</span>squeeze(), attention_mask_ques<span style="color:#f92672">.</span>squeeze()) 
    g <span style="color:#f92672">=</span> model(input_encoding_conv<span style="color:#f92672">.</span>squeeze(), attention_mask_conv<span style="color:#f92672">.</span>squeeze())  

    <span style="color:#75715e"># normalize along the batch dimensions, thus we have the normalized features across all batches</span>
    f_norm <span style="color:#f92672">=</span> (f <span style="color:#f92672">-</span> f<span style="color:#f92672">.</span>mean(<span style="color:#ae81ff">0</span>)) <span style="color:#f92672">/</span> f<span style="color:#f92672">.</span>std(<span style="color:#ae81ff">0</span>)
    g_norm <span style="color:#f92672">=</span> (g <span style="color:#f92672">-</span> g<span style="color:#f92672">.</span>mean(<span style="color:#ae81ff">0</span>)) <span style="color:#f92672">/</span> g<span style="color:#f92672">.</span>std(<span style="color:#ae81ff">0</span>)

    <span style="color:#75715e"># cross-correlation matrix</span>
    c <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>matmul(f_norm<span style="color:#f92672">.</span>T, g_norm)<span style="color:#f92672">/</span> barlow_sample_batch_size

    <span style="color:#75715e"># Barlow Loss</span>
    invariance_term <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>diagonal(c)<span style="color:#f92672">.</span>add_(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>pow_(<span style="color:#ae81ff">2</span>)<span style="color:#f92672">.</span>sum()
    redundancy_reduction_term <span style="color:#f92672">=</span> off_diagonal(c)<span style="color:#f92672">.</span>pow_(<span style="color:#ae81ff">2</span>)<span style="color:#f92672">.</span>sum()
    loss <span style="color:#f92672">=</span> invariance_term <span style="color:#f92672">+</span> lambd <span style="color:#f92672">*</span> redundancy_reduction_term
    <span style="color:#f92672">...</span>
</code></pre></div><p>Now to discuss our implementation. We create a specific dataloader for Barlow that passes through a batch of questions and a batch of conversations with the same <code>answer_id</code>, of equal size. Both batches are passed through the same model (a frozen BERT &amp; a trainable barlow head)</p>
<p>The the cross-correlation matrix c is computed as discussed previously, and finally normalized with the corresponding batch size.</p>
<p><code>c = (f - mean(f))*(g - mean(g))/(std(f) * std(g))/batch_size</code>,</p>
<p>The reason for normalising the cross-correlation matrix is because the batch_size is not constant between new <code>answer_ids</code>. Some <code>answer_ids</code> have only a few questions and conversations, where as others may have hundreds. Moreover, the two classes may be imbalanced for any given <code>answer_id</code>. To solve this, and to maximize the batch sizes, the data_loader first computes the maximal possible batch size for the <code>answer_id</code>.</p>
<p><code>batch_size = min(len(questions), len(conversations))</code></p>
<p>Then it takes all datapoints from the smaller set, and samples from the larger set until we have an equal number of data points from both sets.</p>
<p>Lastly, the <code>invariance_term</code> and <code>redundancy_term</code> are computed using two helper functions, <code>diagonal</code> and <code>off_diagonal</code> that return flattened versions of all elements in the diagonal or off-diagonal. The two terms are implemented exactly as described in <a href="#theory">Theory</a></p>




  



<h3 id="implementation">Implementation</h3><p><strong>Pre-training with inherited head</strong></p>
<p>We initially pretrainined with a Barlow Head and then passed the head direction to the Dual-Encoder Document Ranking Model. We found that while we saw that the shared head was converging very smoothly for the pre-training, and showed excellent results in pushing together the embeddings, it had very poor one-shot performance in Document Ranking, and did not converge more quickly.</p>
<p><strong>Pre-training with frozen head</strong></p>
<p>Consequently, we hypothesized that the model was not sufficiently complex to capture both the Barlow objective (push together question and conversation embeddings) and the Ranking objective (score highly on document ranking). That is, after training for the Barlow objective, the weights in the head were quickly overwritten when it was tasked with optimising for the Ranking objective. To counteract this (and thus get a true evaluation of applying Barlow Twins as a pretraining step), we decided to freeze the Barlow Twins head during training. Thus, the model is forced to rank documents <em>without unlearning</em> the embeddings from the pretraining.</p>
<p>The architecture for Pre-training with frozen head is shown below.</p>

  
    



  



<h1 id="model">Model</h1><blockquote>
<p>The two models are outlined below, where <a style="color:tomato">red</a> indicates sub-models with frozen parameters and where <a style="color:dodgerblue">blue</a> indicates sub-models with trainable parameters.</p>
</blockquote>
<blockquote>
<p><strong>Pre-training Model</strong></p>
<ol>
<li>
<p style="color:tomato"> BERT</p>
</li>
<li>
<p style="color:dodgerblue"> Barlow Head</p>
</li>
</ol>
</blockquote>
<blockquote>
<p><strong>Training Model</strong></p>
<ol>
<li>
<p style="color:tomato"> Pre-training Model</p>
<p style="color:tomato">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Bert<p>
<p style="color:tomato">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Barlow Head</p>
</li>
<li>
<p style="color:dodgerblue"> Ranking Head</p>
</li>
</ol>
</blockquote>
<p>The following section will discuss how the pre-training (with frozen head) and training models are constructed.</p>




  



<h2 id="bert-base">BERT Base</h2><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">BERT</span>(nn<span style="color:#f92672">.</span>Module):
    <span style="color:#66d9ef">def</span> __init__(self, config):

        super(BERT, self)<span style="color:#f92672">.</span>__init__()
        <span style="color:#75715e"># Reference: https://huggingface.co/Maltehb/danish-bert-botxo/blob/main/README.md</span>
        self<span style="color:#f92672">.</span>bert <span style="color:#f92672">=</span> AutoModel<span style="color:#f92672">.</span>from_pretrained(<span style="color:#e6db74">&#34;Maltehb/danish-bert-botxo&#34;</span>)

        <span style="color:#75715e"># freeze all the parameters in BERT. This prevents updating of model weights during fine-tuning</span>
        <span style="color:#66d9ef">for</span> param <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>bert<span style="color:#f92672">.</span>parameters():
            param<span style="color:#f92672">.</span>requires_grad <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, input_id, mask):
        BERT_output <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>bert(input_id, attention_mask<span style="color:#f92672">=</span>mask)
        <span style="color:#66d9ef">return</span> BERT_output
</code></pre></div><p>To delve further into the code, the BERT base simply passes the <code>input_id</code> and <code>attention_masks</code> from the Bert tokeniser through the Danish BERT model, downloaded from <a href="https://huggingface.co/Maltehb/danish-bert-botxo/blob/main/README.md">Hugging Face</a>.</p>
<p>We made an initial choice to only train the head (for time and complexity reasons), therefore in <code>init</code> we freeze all BERT paramters so they will never be updated.</p>




  



<h2 id="barlow-head">Barlow Head</h2><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">barlow_HEAD</span>(nn<span style="color:#f92672">.</span>Module):

    <span style="color:#66d9ef">def</span> __init__(self, config):
        super(barlow_HEAD, self)<span style="color:#f92672">.</span>__init__()
        self<span style="color:#f92672">.</span>dropout <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Dropout(config<span style="color:#f92672">.</span>dropout)
        self<span style="color:#f92672">.</span>relu <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>ReLU()
        self<span style="color:#f92672">.</span>fc1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">768</span>, <span style="color:#ae81ff">768</span>)
        self<span style="color:#f92672">.</span>fc2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">768</span>, <span style="color:#ae81ff">768</span>)
        self<span style="color:#f92672">.</span>bn1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>BatchNorm1d(<span style="color:#ae81ff">768</span>)
        self<span style="color:#f92672">.</span>bn2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>BatchNorm1d(<span style="color:#ae81ff">768</span>)

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, BERT_output):
        cls <span style="color:#f92672">=</span> BERT_output<span style="color:#f92672">.</span>last_hidden_state[:, <span style="color:#ae81ff">0</span>, :]
        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>bn1(self<span style="color:#f92672">.</span>fc1(cls))
        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>relu(x)
        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>dropout(x)
        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>bn2(self<span style="color:#f92672">.</span>fc2(x))
        <span style="color:#66d9ef">return</span> x
</code></pre></div><p>The goal of this model is to push together question and conversation encodings.</p>
<p>We apply the following steps:</p>
<ol>
<li>capture the CLS token from the BERT output.</li>
<li>Pass through a fully connected layer with batch normalisation</li>
<li>Apply a ReLU activation function</li>
<li>Apply Dropout</li>
<li>Pass through a second fully connected layer with batch normalisation</li>
</ol>




  



<h2 id="ranking-head">Ranking Head</h2><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">ranking_HEAD</span>(nn<span style="color:#f92672">.</span>Module):

    <span style="color:#66d9ef">def</span> __init__(self, config):
        super(ranking_HEAD, self)<span style="color:#f92672">.</span>__init__()
        self<span style="color:#f92672">.</span>dropout <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Dropout(config<span style="color:#f92672">.</span>dropout)
        self<span style="color:#f92672">.</span>relu <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>ReLU()
        self<span style="color:#f92672">.</span>fc1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">768</span>, <span style="color:#ae81ff">768</span>)
        self<span style="color:#f92672">.</span>fc2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">768</span>, <span style="color:#ae81ff">768</span>)
        self<span style="color:#f92672">.</span>bn1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>BatchNorm1d(<span style="color:#ae81ff">768</span>)
        self<span style="color:#f92672">.</span>bn2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>BatchNorm1d(<span style="color:#ae81ff">768</span>)

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>bn1(self<span style="color:#f92672">.</span>fc1(x))
        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>relu(x)
        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>dropout(x)
        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>bn2(self<span style="color:#f92672">.</span>fc2(x))
        <span style="color:#66d9ef">return</span> x

</code></pre></div><p>The goal of this model is to pair question and conversation encodings with document encodings to optimise the Document Ranking score.</p>
<p>We apply the following steps:</p>
<ol>
<li>Pass through a fully connected layer with batch normalisation</li>
<li>Apply a ReLU activation function</li>
<li>Apply Dropout</li>
<li>Pass through a fully connected layer with batch normalisation</li>
</ol>




  



<h2 id="pre-training-model">Pre-training Model</h2><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">pretrainingModel</span>(nn<span style="color:#f92672">.</span>Module):

    <span style="color:#66d9ef">def</span> __init__(self, config):
        super(pretrainingModel, self)<span style="color:#f92672">.</span>__init__()
        self<span style="color:#f92672">.</span>bert <span style="color:#f92672">=</span> BERT(config)
        self<span style="color:#f92672">.</span>barlow_HEAD <span style="color:#f92672">=</span> barlow_HEAD(config)

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, input_id, mask):
        BERT_output <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>bert(input_id, mask)
        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>barlow_HEAD(BERT_output)
        <span style="color:#66d9ef">return</span> x
</code></pre></div><p>The pre-training model simply passes the <code>input_id</code> and <code>mask</code> from the tokeniser through the bert model and the barlow head. The BERT parameters are frozen in the <a href="#bert-base">Bert Base</a> <code>init</code>, so only the <a href="#barlow-head">Barlow Head</a> parameters train.</p>




  



<h2 id="training-model">Training Model</h2><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">trainingModel</span>(nn<span style="color:#f92672">.</span>Module):

    <span style="color:#66d9ef">def</span> __init__(self, pretrainingModel: pretrainingModel, config):
        super(trainingModel, self)<span style="color:#f92672">.</span>__init__()

        <span style="color:#75715e"># inherit the BERT and BarlowHEAD from pretraining model</span>
        self<span style="color:#f92672">.</span>pretrainingModel <span style="color:#f92672">=</span> pretrainingModel
        <span style="color:#75715e"># freeze all the parameters in barlow_HEAD. </span>
        <span style="color:#75715e"># This embedding head shouldn&#39;t learn a new task. </span>
        <span style="color:#75715e"># Ranking head needs to adapt to embedding provided.</span>
        <span style="color:#66d9ef">for</span> param <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>pretrainingModel<span style="color:#f92672">.</span>parameters():
            param<span style="color:#f92672">.</span>requires_grad <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>
            
        self<span style="color:#f92672">.</span>ranking_HEAD <span style="color:#f92672">=</span> ranking_HEAD(config)
    
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">set_pretrain_model_to_eval</span>(self):
        self<span style="color:#f92672">.</span>pretrainingModel<span style="color:#f92672">.</span>eval()
    
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, input_id, mask):
        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>pretrainingModel(input_id, mask)
        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>ranking_HEAD(x)
        <span style="color:#66d9ef">return</span> x
</code></pre></div><p>In the Training-Model, the <a href="#pre-training-model">pre-training model</a> (including both <a href="#bert-base">Bert Base</a> and <a href="#barlow-head">Barlow Head</a>) is passed as a parameter. All model parameters are frozen, such that the training model will not update parameters from the pre-training model. During a forward pass, input is passed through the frozen pre-training model and then through an addition <a href="#ranking-head">Ranking Head</a> which is trained with the objective of optimising document ranking scores.</p>
<p>Moreover, the training-model includes a method <code>set_pretrain_model_to_eval</code> that is called from the main script. When the Training Model is set to train mode, we want to ensure that only the ranking head is actually in train mode, while all frozen pre-training sub-models remain in eval mode. Thus, <code>set_pretrain_model_to_eval</code> sets the <a href="#pre-training-model">pre-training model</a> back to eval.</p>

  
    



  



<h1 id="discussion">Discussion</h1><p>The main takeaway was that we could see that both the Document Ranking BERT and the Barlow models worked seperately. However, as soon as we coupled them, the model performed significantly worse than the original baseline. In fact, the starting accuracy was nearly 0, giving the same performance as randomly guessing.</p>
<p>We tried with both frozen and unfrozen parameters being passed from the pretraining model to the training model, but in neither case could we see any improvement. We also hypothesized that this may be because the scaling is off, meaning that the embeddings are in an entirely different scale during pre-training than during training. This led us to implement batch-normalization in all the models to see if there was any improvement. However, the model did not improve.</p>
<p>Firstly, we consider the possibility that the two models might just not be compatible. Essentially, maybe pushing the embeddings together is simply an inherently poor idea based on a faulty assumption that questions and conversations with the same <code>answer id</code> should lie in the same latent space. This might be a very human intuition that we are imposing on the model, which may not necessarily translate into computer understanding. In fact, we do not even know how closely the two concepts of distance and relative ranking are linked. Potentially there is no correlation at all, and pushing the embeddings together is just an additional &ldquo;hurdle&rdquo; for the model to overcome. We propose that this be studied in Future Work, as an indicator for whether this project should be continued.</p>
<p>We notice that the pretrained model is more or less broken for the ranking task. This hints to the fact that there might be a fundamental problem in the integration between the two models. Raffle.ai has had previous experience with BERT&rsquo;s apparent resistance to pretraining tasks. In other words, there might be a need for a more careful integration, rather than simply passing forward the trained BARLOW head put ontop of BERT. Given our time constraints, these considerations were beyond the scope of this project. We propose this for the future work.</p>

  

      </div>
      <div class="dark-box">
        
          <div class="lang-selector" x-data="langController" x-init="initLangs([{&#34;key&#34;:&#34;python&#34;,&#34;name&#34;:&#34;Python&#34;}])">
            <template x-for="(tab, index) in tabs">
              <a x-text="tab.name" :class="{ 'active': tab.active }" @click="changeLanguage(index)"></a>
            </template>
          </div>
        
      </div>
    </div>
    
  </body>
</html>
