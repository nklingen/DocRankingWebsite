<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Handover Document</title>
    <link>https://nklingen.github.io/DocRankingWebsite/</link>
    <description>Recent content on Handover Document</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="https://nklingen.github.io/DocRankingWebsite/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Intro</title>
      <link>https://nklingen.github.io/DocRankingWebsite/docs/intro/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://nklingen.github.io/DocRankingWebsite/docs/intro/</guid>
      <description>Intro AboutThis website is a Handover Document by Mikkel Odgaard and Natasha Klingenbrunn for a collaboration project between Denmark&amp;rsquo;s Technical University and Raffle.ai. The final code can be found on Github.
The website is broken down as follows:
 A brief conceptual background is given for Barlow Twins and BERT. Next, the Timeline section will discuss the evolution of our project and discuss the rationale behind given design choices. The code break-down for our model and barlow loss can be found in their respective sections.</description>
    </item>
    
    <item>
      <title>Barlow</title>
      <link>https://nklingen.github.io/DocRankingWebsite/docs/barlow/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://nklingen.github.io/DocRankingWebsite/docs/barlow/</guid>
      <description>Barlow IntroBarlow Twins takes a batch of samples, applies noise to generate two distored versions, then passes both versions through two identical networks to get their corresponding embeddings. The Barlow loss is then computed on the embeddings, wherein the goal is to get the cross-correlation matrix between the embeddings as close as possible to the identity matrix. In this way, the embeddings of the two versions of the sample are encourraged to be similar, while redundancy between the components of the vectors is penalized.</description>
    </item>
    
    <item>
      <title>BERT</title>
      <link>https://nklingen.github.io/DocRankingWebsite/docs/bert/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://nklingen.github.io/DocRankingWebsite/docs/bert/</guid>
      <description> BERTNat : Brief section on how BERT was implemented  </description>
    </item>
    
    <item>
      <title>Model</title>
      <link>https://nklingen.github.io/DocRankingWebsite/docs/model/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://nklingen.github.io/DocRankingWebsite/docs/model/</guid>
      <description>Model The two models are outlined below, where red indicates sub-models with frozen parameters and where blue indicates sub-models with trainable parameters.
  Pre-training Model
  BERT
  Barlow Head
    Training Model
  Pre-training Model
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; Bert &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; Barlow Head
  Ranking Head
   To re-iterated, pre-training a Barlow head and then inheriting the head directly in the training loop led to poor results.</description>
    </item>
    
    <item>
      <title>Metrics</title>
      <link>https://nklingen.github.io/DocRankingWebsite/docs/metrics/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://nklingen.github.io/DocRankingWebsite/docs/metrics/</guid>
      <description># Computes top k accuracy def k_accuracy(scores, answer_ids, target_answer_ids, k): assert k &amp;gt;= 1 if k == 1: prediction_indices = torch.argmax(scores, dim=1) prediction_answer_ids = answer_ids[prediction_indices] return (prediction_answer_ids == target_answer_ids) elif k &amp;gt; 1: prediction_indices = torch.topk(scores, k).indices prediction_answer_ids = answer_ids[prediction_indices] return (prediction_answer_ids == target_answer_ids.unsqueeze(1)).any(1) # Computes top k average precision score def batch_map(relevance, k): top_k = relevance[:, :k] batch_size = top_k.size(0) pos = torch.arange(1, top_k.size(1) + 1, dtype=torch.float).unsqueeze(0).repeat(batch_size, 1).to(top_k.device) csum, num_ans = top_k.</description>
    </item>
    
    <item>
      <title>Timeline</title>
      <link>https://nklingen.github.io/DocRankingWebsite/docs/timeline/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://nklingen.github.io/DocRankingWebsite/docs/timeline/</guid>
      <description>TimelineThe project went through 4 overall phases in regards to the construction of the model.
 Fundamental construction Exploratory Data Analysis (EDA) Generic improvements Barlow Twins  We will skip the majority of the details and focus on the key takeaways of each phase leading up to the final version of our model. This section will thus work as a brief summary of our work.
1. Fundamental constructionWithout going into much detail, the first part of the project was the implementation, and understandment, of BERT.</description>
    </item>
    
    <item>
      <title>Discussion</title>
      <link>https://nklingen.github.io/DocRankingWebsite/docs/discussion/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://nklingen.github.io/DocRankingWebsite/docs/discussion/</guid>
      <description>DiscussionNat  We can see that Barlow Head accomplishes what it needs to do. (it works) but still that it breaks the downstream BERT task. We tried both frozen and unfrozen heads, and in both cases BERT is much worse off with the pretraining than with random initialized weights. We made sure it can&amp;rsquo;t be a scaling issue as everything is normalized.
Take-away, BERT makes its own representations in the encoding space that work well for document ranking, and tweaking these seems to hurt the model.</description>
    </item>
    
  </channel>
</rss>
