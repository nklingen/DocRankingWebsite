<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Handover Document</title>
    <link>https://nklingen.github.io/DocRankingWebsite/</link>
    <description>Recent content on Handover Document</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="https://nklingen.github.io/DocRankingWebsite/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Intro</title>
      <link>https://nklingen.github.io/DocRankingWebsite/docs/intro/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://nklingen.github.io/DocRankingWebsite/docs/intro/</guid>
      <description> IntroIntroduction / Motivation  </description>
    </item>
    
    <item>
      <title>Barlow</title>
      <link>https://nklingen.github.io/DocRankingWebsite/docs/barlow/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://nklingen.github.io/DocRankingWebsite/docs/barlow/</guid>
      <description>Barlow IntroBarlow Twins takes a batch of samples, applies noise to generate two distored versions, then passes both versions through two identical networks to get their corresponding embeddings. The Barlow loss is then computed on the embeddings, wherein the goal is to get the cross-correlation matrix between the embeddings as close as possible to the identity matrix. In this way, the embeddings of the two versions of the sample are encourraged to be similar, while redundancy between the components of the vectors is penalized.</description>
    </item>
    
    <item>
      <title>BERT</title>
      <link>https://nklingen.github.io/DocRankingWebsite/docs/bert/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://nklingen.github.io/DocRankingWebsite/docs/bert/</guid>
      <description> BERTNat : Brief section on how BERT was implemented  </description>
    </item>
    
    <item>
      <title>Model</title>
      <link>https://nklingen.github.io/DocRankingWebsite/docs/model/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://nklingen.github.io/DocRankingWebsite/docs/model/</guid>
      <description>Model The two models are outlined below, where red indicates sub-models with frozen parameters and where blue indicates sub-models with trainable parameters.
  Pre-training Model
  BERT
  Barlow Head
    Training Model
  Pre-training Model
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; Bert &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; Barlow Head
  Ranking Head
   To re-iterated, pre-training a Barlow head and then inheriting the head directly in the training loop led to poor results.</description>
    </item>
    
    <item>
      <title>Timeline</title>
      <link>https://nklingen.github.io/DocRankingWebsite/docs/timeline/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://nklingen.github.io/DocRankingWebsite/docs/timeline/</guid>
      <description> TimelineMikkel&#39;s portion on what we tried </description>
    </item>
    
    <item>
      <title>Discussion</title>
      <link>https://nklingen.github.io/DocRankingWebsite/docs/discussion/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://nklingen.github.io/DocRankingWebsite/docs/discussion/</guid>
      <description>DiscussionNat \ Mikkel  We can see that Barlow Head accomplishes what it needs to do. (it works) but still that it breaks the downstream BERT task. We tried both frozen and unfrozen heads, and in both cases BERT is much worse off with the pretraining than with random initialized weights. We made sure it can&amp;rsquo;t be a scaling issue as everything is normalized.
Take-away, BERT makes its own representations in the encoding space that work well for document ranking, and tweaking these seems to hurt the model.</description>
    </item>
    
  </channel>
</rss>
