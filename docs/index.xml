<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>DocuAPI Example Site</title>
    <link>https://nklingen.github.io/DocRankingWebsite/</link>
    <description>Recent content on DocuAPI Example Site</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="https://nklingen.github.io/DocRankingWebsite/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Model</title>
      <link>https://nklingen.github.io/DocRankingWebsite/docs/model/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://nklingen.github.io/DocRankingWebsite/docs/model/</guid>
      <description>ModelTo re-iterated, pre-training a Barlow head and then inheriting the head directly in the training loop led to poor results. While we saw that the shared head was converging very smoothly for the initial pre-training with Barlow Loss, and showed excellent results in pushing together the embeddings, it had very poor one-shot performance in Document Ranking, and did not converge more quickly.Consequently, we hypothesized that the model was not sufficiently complex to capture both the Barlow objective (push together question and conversation embeddings) and the Ranking objective (score highly on document ranking).</description>
    </item>
    
    <item>
      <title>Barlow</title>
      <link>https://nklingen.github.io/DocRankingWebsite/docs/barlow/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://nklingen.github.io/DocRankingWebsite/docs/barlow/</guid>
      <description>BarlowThe original paper, Barlow Twins: Self-Supervised Learning via Redundancy Reduction proposes a loss function to encourage a model to create embeddings that are invariant to distortion.
&#34;Barlow Twins strongly benefits from the use of very high-dimensional embeddings&#34; and &#34;does not require large batches&#34;  Their architecture is as follows (simplified explanation):
 take an input image and apply distortions to it to feed the distorted images through an encoder network and a projector network to get their respective embeddings compute the Barlow Loss between the embeddings.</description>
    </item>
    
  </channel>
</rss>
