<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Handover Document</title>
    <link>https://nklingen.github.io/DocRankingWebsite/</link>
    <description>Recent content on Handover Document</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="https://nklingen.github.io/DocRankingWebsite/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Intro</title>
      <link>https://nklingen.github.io/DocRankingWebsite/docs/intro/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://nklingen.github.io/DocRankingWebsite/docs/intro/</guid>
      <description>Intro AboutThis website is a Handover Document by Mikkel Odgaard and Natasha Klingenbrunn for a collaboration project between Denmark&amp;rsquo;s Technical University and Raffle.ai. The final code can be found on Github.
The website is broken down as follows:
 A brief conceptual background is given for Barlow Twins and BERT. Next, the Timeline section will discuss the evolution of our project and discuss the rationale behind given design choices. The code break-down for our model and barlow loss can be found in their respective sections.</description>
    </item>
    
    <item>
      <title>Timeline</title>
      <link>https://nklingen.github.io/DocRankingWebsite/docs/timeline/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://nklingen.github.io/DocRankingWebsite/docs/timeline/</guid>
      <description>TimelineThis section will be a general summary of our work and the evolution of the project. The project went through 4 overall phases in regards to the construction of the model.
Exploratory Data Analysis (EDA) Document Ranking Model Barlow Twins  Exploratory Data AnalysisThe input data contains 4 main pieces of information. The title, the content, the type (questions or conversations) and answer id (the &amp;ldquo;true&amp;rdquo; label). Each piece of information should be analysed due to its importance.</description>
    </item>
    
    <item>
      <title>Metrics</title>
      <link>https://nklingen.github.io/DocRankingWebsite/docs/metrics/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://nklingen.github.io/DocRankingWebsite/docs/metrics/</guid>
      <description>Metrics# Computes top k accuracy def k_accuracy(scores, answer_ids, target_answer_ids, k): assert k &amp;gt;= 1 if k == 1: prediction_indices = torch.argmax(scores, dim=1) prediction_answer_ids = answer_ids[prediction_indices] return (prediction_answer_ids == target_answer_ids) elif k &amp;gt; 1: prediction_indices = torch.topk(scores, k).indices prediction_answer_ids = answer_ids[prediction_indices] return (prediction_answer_ids == target_answer_ids.unsqueeze(1)).any(1) # Computes top k average precision score def batch_map(relevance, k): top_k = relevance[:, :k] batch_size = top_k.size(0) pos = torch.arange(1, top_k.size(1) + 1, dtype=torch.float).unsqueeze(0).repeat(batch_size, 1).to(top_k.device) csum, num_ans = top_k.</description>
    </item>
    
    <item>
      <title>Model</title>
      <link>https://nklingen.github.io/DocRankingWebsite/docs/model/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://nklingen.github.io/DocRankingWebsite/docs/model/</guid>
      <description>Model The two models are outlined below, where red indicates sub-models with frozen parameters and where blue indicates sub-models with trainable parameters.
  Pre-training Model
  BERT
  Barlow Head
    Training Model
  Pre-training Model
&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; Bert &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; Barlow Head
  Ranking Head
   To re-iterated, pre-training a Barlow head and then inheriting the head directly in the training loop led to poor results.</description>
    </item>
    
    <item>
      <title>Discussion</title>
      <link>https://nklingen.github.io/DocRankingWebsite/docs/discussion/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://nklingen.github.io/DocRankingWebsite/docs/discussion/</guid>
      <description>DiscussionNat  We can see that Barlow Head accomplishes what it needs to do. (it works) but still that it breaks the downstream BERT task. We tried both frozen and unfrozen heads, and in both cases BERT is much worse off with the pretraining than with random initialized weights. We made sure it can&amp;rsquo;t be a scaling issue as everything is normalized.
Take-away, BERT makes its own representations in the encoding space that work well for document ranking, and tweaking these seems to hurt the model.</description>
    </item>
    
  </channel>
</rss>
